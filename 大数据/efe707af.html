<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>消息队列Kafka | 风雨欲来兮丶</title><meta name="author" content="SeanXia"><meta name="copyright" content="SeanXia"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Kafka 是一个高吞吐、低延迟分布式的消息队列系统。kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒。 官网：https:&#x2F;&#x2F;kafka.apache.org">
<meta property="og:type" content="article">
<meta property="og:title" content="消息队列Kafka">
<meta property="og:url" content="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/efe707af.html">
<meta property="og:site_name" content="风雨欲来兮丶">
<meta property="og:description" content="Kafka 是一个高吞吐、低延迟分布式的消息队列系统。kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒。 官网：https:&#x2F;&#x2F;kafka.apache.org">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg">
<meta property="article:published_time" content="2017-10-27T16:00:00.000Z">
<meta property="article:modified_time" content="2019-08-10T13:46:42.000Z">
<meta property="article:author" content="SeanXia">
<meta property="article:tag" content="Kafka">
<meta property="article:tag" content="消息队列">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg"><link rel="shortcut icon" href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/LOGO%E9%80%8F%E6%98%8E.png"><link rel="canonical" href="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/efe707af.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: SeanXia","link":"链接: ","source":"来源: 风雨欲来兮丶","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '消息队列Kafka',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2019-08-10 21:46:42'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="风雨欲来兮丶" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">72</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/butterfly.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="风雨欲来兮丶"><span class="site-name">风雨欲来兮丶</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">消息队列Kafka</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2017-10-27T16:00:00.000Z" title="发表于 2017-10-28 00:00:00">2017-10-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2019-08-10T13:46:42.000Z" title="更新于 2019-08-10 21:46:42">2019-08-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id data-flag-title="消息队列Kafka"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Kafka 是一个高吞吐、低延迟分布式的消息队列系统。kafka 每秒可以处理几十万条消息，它的延迟最低只有几毫秒。</p>
<p>官网：<a href="https://kafka.apache.org/" rel="external nofollow noopener noreferrer" target="_blank">https://kafka.apache.org</a></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmjytyjtrj20bg091gm6.jpg"></p>
<span id="more"></span>

<h1 id="Kafka-简介"><a href="#Kafka-简介" class="headerlink" title="Kafka 简介"></a>Kafka 简介</h1><h2 id="Kafka-架构"><a href="#Kafka-架构" class="headerlink" title="Kafka 架构"></a>Kafka 架构</h2><blockquote>
<ul>
<li>kafka 集群有多个 <code>Broker</code> 服务器组成，每个类型的消息被定义为 <code>topic</code>。</li>
<li>同一 topic 内部的消息按照一定的 key 和算法被分区(<code>partition</code>)存储在不同的 Broker 上。</li>
<li>消息生产者 <code>producer</code> 和消费者 <code>consumer</code> 可以在多个 Broker 上生产&#x2F;消费 topic。</li>
</ul>
</blockquote>
<p><strong>Broker：服务器节点</strong></p>
<p>消息中间件处理节点，一个Kafka节点就是一个broker，一个或者多个Broker可以组成一个Kafka集群；</p>
<p><strong>Topic：消息主题（类型）</strong></p>
<p>主题是对一组消息的抽象分类，比如例如page view日志、click日志等都可以以topic的形式进行抽象划分类别。在物理上，不同Topic的消息分开存储，逻辑上一个Topic的消息虽然保存于一个或多个broker上但用户只需指定消息的Topic即可使得数据的生产者或消费者不必关心数据存于何处；</p>
<p>Topic 即为每条发布到 Kafka 集群的消息的类别，topic 在 Kafka 中可以由多个消费者订阅、消费。</p>
<p>一个 topic 可以有多个 partition，分布在不同的 broker 服务器上。</p>
<p><strong>Partiton：分区</strong></p>
<p>每个主题又被分成一个或者若干个分区（Partition）。每个分区在本地磁盘上对应一个文件夹，分区命名规则为主题名称后接 “ - ” 连接符，之后再接分区编号，分区编号从0开始至分区总数减-1。</p>
<blockquote>
<p><strong>强一致性：</strong><code>Kafka 只保证一个分区内的消息有序</code>，不能保证一个主题的不同分区之间的消息有序。<code>如果你想要保证所有的消息都绝对有序可以只为一个主题分配一个分区。</code></p>
</blockquote>
<p><strong>Producers ：生产者</strong></p>
<p>消息生产者，负责发布消息到 Kafka broker。</p>
<p><strong>Consumers：消费者</strong></p>
<p>消息消费者，向 Kafka broker 读取消息的客户端。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmkc7ydhfj20fj07t77z.jpg"></p>
<h2 id="Kafka-中其他关键词"><a href="#Kafka-中其他关键词" class="headerlink" title="Kafka 中其他关键词"></a>Kafka 中其他关键词</h2><p><strong>LogSegment：日志分段</strong></p>
<p>在Kafka中，每个分区又被划分为多个日志分段（LogSegment）组成，日志分段是Kafka日志对象分片的最小单位 LogSegment 算是一个逻辑概念，对应一个具体的日志文件（“.log”的数据文件）和两个索引文件（“.index”和“.timeindex”，分别表示偏移量索引文件和消息时间戳索引文件）组成。</p>
<p><strong>Offset：消息偏移量</strong></p>
<p>每个 partition 中都由一系列有序的、不可变的消息组成，这些消息被顺序地追加到partition中。每个消息都有一个连续的序列号称之为 offset 消息偏移量，用于在 partition 内唯一标识消息（并不表示消息在磁盘上的物理位置）。</p>
<p>分区会给每个消息记录分配一个顺序 ID 号（<code>偏移量offset</code>）， 能够唯一地标识该分区中的每个记录。Kafka 集群保留所有发布的记录，不管这个记录有没有被消费过，<code>Kafka 提供相应策略通过配置从而对旧数据处理。</code></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmk7tl02qj20e507ddg4.jpg"></p>
<p>实际上，每个消费者唯一保存的<code>元数据信息</code>就是消费者当前消费日志的位移位置。<code>位移位置是由消费者控制</code>，即、消费者可以通过修改偏移量读取任何位置的数据。</p>
<p><strong>Messeage：消息</strong></p>
<p>消息是Kafka中存储的最小最基本的单位，即为一个commit log，由一个固定长度的消息头和一个可变长度的消息体组成。</p>
<h2 id="Kafka-的使用场景"><a href="#Kafka-的使用场景" class="headerlink" title="Kafka 的使用场景"></a>Kafka 的使用场景</h2><p><strong>日志收集：</strong></p>
<p>一个公司可以用Kafka可以收集各种服务的log，通过 kafka 以统一接口服务的方式开放给各种 consumer，例如hadoop、Hbase、Solr 等。</p>
<p><strong>消息系统：</strong></p>
<p>解耦和生产者和消费者、缓存消息等。</p>
<p><strong>用户活动跟踪：</strong></p>
<p>Kafka 经常被用来记录 web 用户或者 app 用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到 kafka 的 topic 中，然后订阅者通过订阅这 topic 来做实时的监控分析，或者装载到 hadoop、数据仓库中做离线分析和挖掘。</p>
<p><strong>运营指标：</strong></p>
<p>Kafka 也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。</p>
<p><strong>流式处理：</strong></p>
<p>比如 spark streaming 和 storm。</p>
<h1 id="Kafka-集群部署"><a href="#Kafka-集群部署" class="headerlink" title="Kafka 集群部署"></a>Kafka 集群部署</h1><p><strong>集群规划：</strong></p>
<p>Zookeeper 集群共三台服务器，分别为：sean01、sean02、sean03。</p>
<p>Kafka 集群共三台服务器，分别为：sean01、sean02、sean03。</p>
<p><strong>1、Zookeeper 集群准备</strong></p>
<p>kafka 是一个分布式消息队列，需要依赖 ZooKeeper，请先安装好 ZooKeeper 集群。</p>
<p><strong>2、安装 Kafka</strong></p>
<p>（1）下载压缩包（官网地址：<a href="http://kafka.apache.org/downloads.html%EF%BC%89" rel="external nofollow noopener noreferrer" target="_blank">http://kafka.apache.org/downloads.html）</a></p>
<p>（2）解压：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka_2.10-0.9.0.1.tgz -C ../	 <span class="comment"># &quot;-C&quot;的作用是解压到指定路径</span></span><br></pre></td></tr></table></figure>

<p>（3）修改配置文件：config&#x2F;server.properties</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmkmm2q5mj20if06xmxb.jpg"></p>
<p>核心配置参数说明：</p>
<p><strong>broker.id：</strong>broker 集群中唯一标识 id，0、1、2、3 依次增长（broker 即 Kafka 集群中的一台服务器）。</p>
<p><code>注：当前 Kafka 集群共三台节点，分别为：sean01、sean02、sean03。对应的 broker.id 分别为 0、1、2。</code></p>
<p><strong>zookeeper.connect：</strong>zookeeper 集群地址列表。</p>
<p>（4）最后将当前服务器上的 Kafka 目录同步到其他服务器节点上。</p>
<p><strong>3、启动 Kafka 集群</strong></p>
<ul>
<li>启动 Zookeeper 集群。</li>
<li>启动 Kafka 集群。</li>
</ul>
<p>分别在三台服务器上执行以下命令启动：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<p><strong>4、测试</strong></p>
<p>（1）创建 topic：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper sean01:2181,sean02:2181,sean03:2181 --create --replication-factor 2 --partitions 3 --topic test</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参数说明：<br>replication-factor：指定每个分区的复制因子个数，默认 1 个<br>partitions：指定当前创建的 kafka 分区数量，默认为 1 个<br>topic：指定新建 topic 的名称</p>
</blockquote>
<p>（2）查看 topic 列表：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper sean01:2181,sean02:2181,sean03:2181 --list</span><br></pre></td></tr></table></figure>

<p>（3）查看 “test” topic 描述：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper sean01:2181,sean02:2181,sean03:2181 --describe --topic test</span><br></pre></td></tr></table></figure>

<p>（4）创建生产者：sean03</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list sean01:9092,sean02:9092,sean03:9092 --topic test</span><br></pre></td></tr></table></figure>

<p>（5）创建消费者：sean02</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper sean01:2181,sean02:2181,sean03:2181 --from-beginning --topic test</span><br></pre></td></tr></table></figure>

<p><strong>注：</strong><br>查看帮助手册：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh help</span><br></pre></td></tr></table></figure>

<p>（6）查看结果：</p>
<p>先在生产者节点sean03中输入几句话</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzml68pn0gj20hd044t8n.jpg"></p>
<p>然后到消费者节点sean02中查看</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmlh9wchpj20m504uaa1.jpg"></p>
<h1 id="Flume与Kafka-整合"><a href="#Flume与Kafka-整合" class="headerlink" title="Flume与Kafka 整合"></a>Flume与Kafka 整合</h1><p><strong>1、Flume 安装</strong></p>
<p>Flume 详细的安装流程详见：[Flume框架][<a href="https://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/93326c6d.html]">https://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/93326c6d.html]</a></p>
<p><strong>2、Flume + Kafka</strong></p>
<ul>
<li>启动 Kafka 集群。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure>

<ul>
<li>配置 Flume 集群，并启动 Flume 集群。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/flume-ng agent -n a1 -c conf -f conf/fk.conf -Dflume.root.logger=DEBUG,console</span><br></pre></td></tr></table></figure>

<p>其中，Flume 配置文件 fk.conf 内容如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = avro</span><br><span class="line">a1.sources.r1.bind = sean03</span><br><span class="line">a1.sources.r1.port = 41414</span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink</span><br><span class="line">a1.sinks.k1.topic = testflume</span><br><span class="line">a1.sinks.k1.brokerList = sean01:9092,sean02:9092,sean03:9092</span><br><span class="line">a1.sinks.k1.requiredAcks = 1</span><br><span class="line">a1.sinks.k1.batchSize = 20</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000000</span><br><span class="line">a1.channels.c1.transactionCapacity = 10000</span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure>

<p><strong>3、测试</strong></p>
<ul>
<li><p>分别启动 Zookeeper、Kafka、Flume 集群。</p>
</li>
<li><p>创建 topic：</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用JavaAPI插入数据时可以不创建，系统会自动创建一个分区和副本都为1的指定Topic</span></span><br><span class="line">bin/kafka-topics.sh --zookeeper sean01:9092,sean02:9092,sean03:9092 --create --replication-factor 2 --partitions 3 --topic testflume</span><br></pre></td></tr></table></figure>

<ul>
<li>启动消费者：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这里我们也可以使用JavaAPI操作，不用shell操作</span></span><br><span class="line">bin/kafka-console-consumer.sh --zookeeper sean01:9092,sean02:9092,sean03:9092 --from-beginning --topic testflume</span><br></pre></td></tr></table></figure>

<ul>
<li>运行 “RpcClientDemo” 代码，通过 RPC 请求发送数据到 Flume 集群。</li>
</ul>
<blockquote>
<p>Flume 中 source 类型为 AVRO 类型，此时通过 Java 发送 rpc 请求，测试数据是否传入 Kafka。</p>
<p>相关Demo可以参考 Flume 官方文档：<a href="http://flume.apache.org/FlumeDeveloperGuide.html" rel="external nofollow noopener noreferrer" target="_blank">http://flume.apache.org/FlumeDeveloperGuide.html</a></p>
</blockquote>
<p><strong>先定义生产者：RpcClientDemo类</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RpcClientDemo</span> &#123;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException &#123;</span><br><span class="line">		<span class="type">MyRpcClientFacade</span> <span class="variable">client</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MyRpcClientFacade</span>();</span><br><span class="line">		<span class="comment">// Initialize client with the remote Flume agent&#x27;s host and port</span></span><br><span class="line">        <span class="comment">// 使用远程Flume代理主机和端口初始化客户端</span></span><br><span class="line">		client.init(<span class="string">&quot;sean03&quot;</span>, <span class="number">41414</span>);</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Send 10 events to the remote Flume agent. That agent should be</span></span><br><span class="line">		<span class="comment">// configured to listen with an AvroSource.</span></span><br><span class="line">		<span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span><span class="number">0</span>; i &lt; <span class="number">300</span>; i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">number</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Random</span>().nextInt(<span class="number">3</span>);</span><br><span class="line">            String sampleData ;</span><br><span class="line">            <span class="keyword">if</span>(number == <span class="number">0</span>)&#123;</span><br><span class="line">                sampleData  = <span class="string">&quot;Hello Flume! ERROR   &quot;</span> + i;</span><br><span class="line">            &#125;<span class="keyword">else</span> <span class="keyword">if</span>(number==<span class="number">1</span>)&#123;</span><br><span class="line">                sampleData  = <span class="string">&quot;Hello Flume! INFO   &quot;</span> + i;</span><br><span class="line">            &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">                sampleData  = <span class="string">&quot;Hello Flume! WARNING   &quot;</span> + i;</span><br><span class="line">            &#125;</span><br><span class="line">			client.sendDataToFlume(sampleData);</span><br><span class="line">			Thread.sleep(<span class="number">500</span>);</span><br><span class="line">		&#125;</span><br><span class="line">		client.cleanUp();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyRpcClientFacade</span> &#123;</span><br><span class="line">	<span class="keyword">private</span> RpcClient client;</span><br><span class="line">	<span class="keyword">private</span> String hostname;</span><br><span class="line">	<span class="keyword">private</span> <span class="type">int</span> port;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">init</span><span class="params">(String hostname, <span class="type">int</span> port)</span> &#123;</span><br><span class="line">		<span class="comment">// Setup the RPC connection</span></span><br><span class="line">		<span class="built_in">this</span>.hostname = hostname;</span><br><span class="line">		<span class="built_in">this</span>.port = port;</span><br><span class="line">		<span class="built_in">this</span>.client = RpcClientFactory.getDefaultInstance(hostname, port);</span><br><span class="line">		<span class="comment">// Use the following method to create a thrift client (instead of the</span></span><br><span class="line">		<span class="comment">// above line):</span></span><br><span class="line">		<span class="comment">// this.client = RpcClientFactory.getThriftInstance(hostname, port);</span></span><br><span class="line">	&#125;</span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendDataToFlume</span><span class="params">(String data)</span> &#123;</span><br><span class="line">		<span class="comment">// Create a Flume Event object that encapsulates the sample data</span></span><br><span class="line">		<span class="type">Event</span> <span class="variable">event</span> <span class="operator">=</span> EventBuilder.withBody(data, Charset.forName(<span class="string">&quot;UTF-8&quot;</span>));</span><br><span class="line"></span><br><span class="line">		<span class="comment">// Send the event</span></span><br><span class="line">		<span class="keyword">try</span> &#123;</span><br><span class="line">			client.append(event);</span><br><span class="line">		&#125; <span class="keyword">catch</span> (EventDeliveryException e) &#123;</span><br><span class="line">			<span class="comment">// clean up and recreate the client</span></span><br><span class="line">			client.close();</span><br><span class="line">			client = <span class="literal">null</span>;</span><br><span class="line">			client = RpcClientFactory.getDefaultInstance(hostname, port);</span><br><span class="line">			<span class="comment">// Use the following method to create a thrift client (instead of</span></span><br><span class="line">			<span class="comment">// the above line):</span></span><br><span class="line">			<span class="comment">// this.client = RpcClientFactory.getThriftInstance(hostname, port);</span></span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">cleanUp</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="comment">// Close the RPC connection</span></span><br><span class="line">		client.close();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>定义消费者：MyConsumer</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyConsumer</span> <span class="keyword">extends</span> <span class="title class_">Thread</span> &#123;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> ConsumerConnector consumer;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">public</span> <span class="title function_">MyConsumer</span><span class="params">(String topic)</span> &#123;</span><br><span class="line">		consumer = Consumer.createJavaConsumerConnector(createConsumerConfig());</span><br><span class="line">		<span class="built_in">this</span>.topic = topic;</span><br><span class="line">	&#125;</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">static</span> ConsumerConfig <span class="title function_">createConsumerConfig</span><span class="params">()</span> &#123;</span><br><span class="line">		<span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">		props.put(<span class="string">&quot;zookeeper.connect&quot;</span>, <span class="string">&quot;sean01:2181,sean02:2181,sean03:2181&quot;</span>);</span><br><span class="line">		props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;xss01&quot;</span>);</span><br><span class="line">		props.put(<span class="string">&quot;zookeeper.session.timeout.ms&quot;</span>, <span class="string">&quot;400&quot;</span>);</span><br><span class="line">		props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;100&quot;</span>);</span><br><span class="line">        props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;smallest&quot;</span>);</span><br><span class="line"><span class="comment">//        props.put(&quot;auto.commit.enable&quot;,&quot;false&quot;); // 关闭自动提交，开启手动提交</span></span><br><span class="line">       </span><br><span class="line">		<span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">ConsumerConfig</span>(props);</span><br><span class="line">	&#125;</span><br><span class="line">    </span><br><span class="line">	<span class="comment">// push消费方式，服务端推送过来。主动方式是pull</span></span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">run</span><span class="params">()</span> &#123;</span><br><span class="line">		Map&lt;String, Integer&gt; topicCountMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;String, Integer&gt;();</span><br><span class="line">        <span class="comment">//mytopic2</span></span><br><span class="line">		topicCountMap.put(topic, <span class="number">1</span>); <span class="comment">// 描述读取哪个topic，需要几个线程读</span></span><br><span class="line">		Map&lt;String, List&lt;KafkaStream&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt;&gt;&gt; consumerMap = consumer</span><br><span class="line">				.createMessageStreams(topicCountMap);</span><br><span class="line">        <span class="comment">// 每个线程对应于一个KafkaStream</span></span><br><span class="line">        List&lt;KafkaStream&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt;&gt; list = consumerMap.get(topic);  </span><br><span class="line">        <span class="type">KafkaStream</span> <span class="variable">stream</span> <span class="operator">=</span> list.get(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">		ConsumerIterator&lt;<span class="type">byte</span>[], <span class="type">byte</span>[]&gt; it = stream.iterator();</span><br><span class="line">        System.out.println(<span class="string">&quot;xixii................&quot;</span>);</span><br><span class="line">        <span class="keyword">while</span> (it.hasNext())&#123;</span><br><span class="line">            <span class="type">String</span> <span class="variable">data</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">String</span>(it.next().message());</span><br><span class="line">            System.out.println(<span class="string">&quot;开始处理数据 ...:&quot;</span>+ data);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Thread.sleep(<span class="number">500</span>);</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line"><span class="comment">//            System.out.println(&quot;数据处理中...&quot; + data);</span></span><br><span class="line"><span class="comment">//            try &#123;</span></span><br><span class="line"><span class="comment">//                Thread.sleep(2000);</span></span><br><span class="line"><span class="comment">//            &#125; catch (InterruptedException e) &#123;</span></span><br><span class="line"><span class="comment">//                e.printStackTrace();</span></span><br><span class="line"><span class="comment">//            &#125;</span></span><br><span class="line"><span class="comment">//            System.out.println(&quot;处理完数据...&quot; + data);</span></span><br><span class="line"><span class="comment">//            consumer.commitOffsets();</span></span><br><span class="line">        &#125;</span><br><span class="line">			</span><br><span class="line">	&#125;</span><br><span class="line">    </span><br><span class="line">	<span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">		<span class="type">MyConsumer</span> <span class="variable">consumerThread</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">MyConsumer</span>(<span class="string">&quot;testflume&quot;</span>);</span><br><span class="line">		consumerThread.start();</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>先启动 RpcClientDemo</strong></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmmjaq61kj20h706j75z.jpg"></p>
<p><strong>再启动 MyConsumer</strong></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmmif2uzzj20fq06kgn6.jpg"></p>
<p>说明数据已经传入Kafka中，Flume和Kafka整合成功。</p>
<p><strong>Flume &amp; Kafka &amp; Storm（Spark）</strong></p>
<p>利用Flume和Kafka，我们还可以跟流式计算框架Storm或Spark进行整合，来处理工作中的业务需求。</p>
<p>大概流程如下：</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmnuzpc6zj20ht0aj0sq.jpg"></p>
<h1 id="Kafka-对比-HDFS"><a href="#Kafka-对比-HDFS" class="headerlink" title="Kafka 对比 HDFS"></a>Kafka 对比 HDFS</h1><p>1、Kafka 分区里的数据是有序的，读写更快。</p>
<p>2、Kafka 写数据先写入内存中，来保证高吞吐量。</p>
<blockquote>
<p> 吞吐量：单位时间内读写的数据量大小。</p>
</blockquote>
<p>3、因为Kafka的数据先写入内存，一旦在写入过程中服务器宕机，会有丢失数据的风险。</p>
<ul>
<li><p>可以通过增加备份来降低数据丢失的风险（副本机制）。</p>
</li>
<li><p>设置 acks 的值。</p>
<blockquote>
<p>acks &#x3D; 0：tomcat客户端只需发送给leader服务器，无需返回的响应信息。速度最快</p>
<p>acks &#x3D; 1：tomcat客户端需要收到leader服务器返回的响应信息。默认此配置</p>
<p>acks &#x3D; -1 或 all：tomcat客户端需要收到所有副本返回的响应信息。速度最慢</p>
</blockquote>
</li>
</ul>
<p><strong>ISR（in syncronized replication）:</strong> leader候选人机制</p>
<p>所有的副本都是在 ISR 列表中的节点上，来保证数据的完整性。</p>
<p>当 leader 服务器宕机时，Zookeeper 会从 ISR 列表中去选举新的 leader，每个副本节点在同步数据时，由于每个节点负载消耗不同，如果某个节点负载过高，数据无法在短时间内同步，就会从 isr 列表中暂时移除，等它同步正常之后在添加会 isr 列表。</p>
<h1 id="Kafka的数据丢失和重复消费"><a href="#Kafka的数据丢失和重复消费" class="headerlink" title="Kafka的数据丢失和重复消费"></a>Kafka的数据丢失和重复消费</h1><h2 id="发生原因"><a href="#发生原因" class="headerlink" title="发生原因"></a>发生原因</h2><p>原因都起始于偏移量 offset 的提交周期问题。系统默认1分钟。</p>
<h3 id="数据丢失"><a href="#数据丢失" class="headerlink" title="数据丢失"></a>数据丢失</h3><p><strong>生产者 producer</strong></p>
<p>当默认状态下（acks&#x3D;1），客户端提交数据到leader服务器，收到leader的响应信息后结束发送。然后在 leader 进行同步备份节点数据时，leader 服务器宕机，而数据还未备份成功，数据就会丢失。下次客户端来访问就访问不到。</p>
<p><strong>消费者 consumer</strong></p>
<p>客户端提交的间隔比较频繁，数据未处理成功叫提交偏移量，此时服务器宕机，数据就会丢失。</p>
<p>试验一下：API中把自动提交的间隔调小</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmq0xucjkj20ha04xac2.jpg"></p>
<p>然后把API中的提交数据过程打开</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmq249aepj20g607r779.jpg"></p>
<p>启动消费端，查看控制台：在xss5未处理完成的时候断开</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmpzj3jp4j20f705m0u1.jpg"></p>
<p>再次重启，查看：发现从 xss6 开始，xss5被忽略了，但xss5上次的偏移量并未提交完成，数据丢失！</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmq46r8x6j20ee05zq44.jpg"></p>
<h3 id="重复消费"><a href="#重复消费" class="headerlink" title="重复消费"></a>重复消费</h3><p>重复消费存在于消费者中。</p>
<p>客户端自动提交的时间间隔太久，在还未提交前服务器宕机，下次重启后只会从之前记录的偏移量开始消费，就会造成重复消费。</p>
<p>试验一下：API中把自动提交的间隔调的大一些</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmq34xuplj20ht05k0v6.jpg"></p>
<p>看消费端，当小费到 xss15 的时候关掉服务</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmpqf1ek2j20es075ta7.jpg"></p>
<p>然后重新启动消费端，发现还是从 xss1 开始，由于未到一分钟还未提交偏移量，形成重复消费。</p>
<h2 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h2><p><strong>1、调整生产者的 acks 状态</strong></p>
<p>改变acks的值，设置为 -1 或 all</p>
<blockquote>
<p>acks &#x3D; -1 或 all：tomcat客户端需要收到所以副本返回的响应信息。速度最慢</p>
</blockquote>
<p><strong>2、关闭自动提交，使用手动提交方式。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">auto.commit.enable&quot;,&quot;false&quot;</span><br></pre></td></tr></table></figure>

<p>默认配置为打开：true</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmoqae0zyj20j20963yy.jpg"></p>
<p>其他详细配置可以参照官网：<a href="http://kafka.apache.org/090/documentation.html#highlevelconsumerapi" rel="external nofollow noopener noreferrer" target="_blank">http://kafka.apache.org/090/documentation.html#highlevelconsumerapi</a></p>
<blockquote>
<p>注：手动提交虽然能解决 Kafka 的数据丢失和重复消费问题，但是效率速度上相对自动提交降低很多，所以具体问题还需具体分析，应该是在实际业务中去取舍。（比如人口统计重复消费问题可以忽略少量，但是效率第一；再如银行业务是绝对不允许重复消费和数据丢失的，对精确度要求更高）</p>
</blockquote>
<h1 id="Kafka-副本机制"><a href="#Kafka-副本机制" class="headerlink" title="Kafka 副本机制"></a>Kafka 副本机制</h1><p>Kafka的副本机制指的是多个服务端节点对其他节点的 topic 分区的日志进行复制。</p>
<p>当集群中的某个节点出现故障，访问故障节点的请求就会被转移到其他正常节点（这个过程叫做 Reblance）。</p>
<p>Kafka每个主题的每个分区都有一个主副本以及0或多个从副本，从副本保持与主副本的数据同步，当主副本出故障时就会被从副本替代。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g0t8zj2imfj20ll08qgnj.jpg"></p>
<blockquote>
<p>在Kafka中，并不是所有的副本都能拿来替代主副本，所以在Kafka的Leader节点中维护着一个ISR列表，候选人机制。<br>当Leader服务器宕机时，Zookeeper会从ISR列表中去选举新的Leader。每个副本节点在同步数据时，由于负载消耗不同，如果某个节点负载较高，数据无法在短时间内同步，就会从ISR列表中暂时移除，等它正常同步之后，再添加到ISR列表中。</p>
</blockquote>
<h1 id="Kafka-数据存储"><a href="#Kafka-数据存储" class="headerlink" title="Kafka 数据存储"></a>Kafka 数据存储</h1><p>Kafka中的消息是以主题（Topic）为基本单位进行组织的，各个主题之间相互独立。在这里主题只是一个逻辑上的抽象概念，而在实际数据文件的存储中，Kafka中的消息存储在物理上是以一个或多个分区（Partition）构成，每个分区对应本地磁盘上的一个文件夹，每个文件夹内包含了日志索引文件（“.index”和“.timeindex”）和日志数据文件（“.log”）两部分。分区数量可以在创建主题时指定，也可以在创建Topic后进行修改。</p>
<p>同时，Kafka为了实现集群的高可用性，在每个Partition中可以设置有一个或者多个副本（Replica），分区的副本分布在不同的Broker节点上。同时，从副本中会选出一个副本作为Leader，<code>Leader副本负责与客户端进行读写操作。而其他副本作为Follower会从Leader副本上进行数据同步。</code></p>
<h2 id="Kafka中分区-副本的日志文件存储分析"><a href="#Kafka中分区-副本的日志文件存储分析" class="headerlink" title="Kafka中分区&#x2F;副本的日志文件存储分析"></a>Kafka中分区&#x2F;副本的日志文件存储分析</h2><p>每个分区又有1至多个副本，分区的副本分布在集群的不同代理上，以提高可用性。从存储的角度上来说，分区的每个副本在逻辑上可以抽象为一个日志（Log）对象，即分区副本与日志对象是相对应的。下图是在三个Kafka Broker节点所组成的集群中分区的主&#x2F;备份副本的物理分布情况图：</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g0huy3x9hpj20ss0gi3zm.jpg"></p>
<h2 id="Kafka中日志索引和数据文件的存储结构"><a href="#Kafka中日志索引和数据文件的存储结构" class="headerlink" title="Kafka中日志索引和数据文件的存储结构"></a>Kafka中日志索引和数据文件的存储结构</h2><p>在Kafka中，每个 Log 对象又可以划分为多个 LogSegment 文件，每个 LogSegment 文件包括一个日志数据文件和两个索引文件（偏移量索引文件和消息时间戳索引文件）。</p>
<p>其中，每个 LogSegment 中的日志数据文件大小均相等（该日志数据文件的大小可以通过在Kafka Broker的config&#x2F;server.properties配置文件的中的<strong>“log.segment.bytes”</strong>进行设置，默认为1G大小（1073741824字节），在顺序写入消息时如果超出该设定的阈值，将会创建一组新的日志数据和索引文件）。</p>
<p>Kafka将日志文件封装成一个FileMessageSet对象，将偏移量索引文件和消息时间戳索引文件分别封装成OffsetIndex 和 TimerIndex 对象。Log和LogSegment均为逻辑概念，Log是对副本在Broker上存储文件的抽象，而 LogSegment 是对副本存储下每个日志分段的抽象，日志与索引文件才与磁盘上的物理存储相对应；下图为Kafka日志存储结构中的对象之间的对应关系图：</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g0hv07ggqmj20lf0dht94.jpg"></p>
<h2 id="Kafka中Message的存储和查找过程"><a href="#Kafka中Message的存储和查找过程" class="headerlink" title="Kafka中Message的存储和查找过程"></a>Kafka中Message的存储和查找过程</h2><p>Message是按照topic来组织，每个topic可以分成多个的partition。比如：有5个partition的名为为page_visits的topic的目录结构为：</p>
<img src="https://ws1.sinaimg.cn/large/7308598bgy1g165rdzcb4j20gw032aav.jpg">

<p>partition是分段的，每个段叫LogSegment，包括了一个数据文件和一个索引文件。下图是某个partition目录下的文件：</p>
<img src="https://ws1.sinaimg.cn/large/7308598bgy1g165sjfk8mj205r05qdg3.jpg">

<p>可以看到，这个partition有4个LogSegment。</p>
<p><strong>查找Message原理图：</strong></p>
<img src="https://ws1.sinaimg.cn/large/7308598bgy1g165t4r05uj211u0pogr8.jpg">

<p>比如，要查找绝对offset为7的Message：</p>
<ol>
<li>首先是用二分查找确定它是在哪个LogSegment中，自然是在第一个Segment中。</li>
<li>打开这个Segment的index文件，也是用二分查找找到offset小于或者等于指定offset的索引条目中最大的那个offset。自然offset为6的那个索引是我们要找的，通过索引文件我们知道offset为6的Message在数据文件中的位置为9807。</li>
<li>打开数据文件，从位置为9807的那个地方开始顺序扫描直到找到offset为7的那条Message。</li>
</ol>
<p>这套机制是建立在offset是有序的。索引文件被映射到内存中，所以查找的速度还是很快的。</p>
<blockquote>
<p>一句话，Kafka的Message存储采用了分区(partition)，分段(LogSegment)和稀疏索引这几个手段来达到了高效性。</p>
</blockquote>
<h2 id="思考拓展"><a href="#思考拓展" class="headerlink" title="思考拓展"></a>思考拓展</h2><h3 id="Kafka使用磁盘也可以高效读写的原因？"><a href="#Kafka使用磁盘也可以高效读写的原因？" class="headerlink" title="Kafka使用磁盘也可以高效读写的原因？"></a>Kafka使用磁盘也可以高效读写的原因？</h3><p><strong>1、顺序写入</strong></p>
<p>因为硬盘是机械结构，每次读写都会寻址-&gt;写入，其中寻址是一个“机械动作”，它是最耗时的。所以硬盘最“讨厌”随机I&#x2F;O，最喜欢顺序I&#x2F;O。为了提高读写硬盘的速度，Kafka就是使用顺序I&#x2F;O。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1fzmk4og9hqj20dx07x3yo.jpg"></p>
<p>上图就展示了Kafka是如何写入数据的， <strong>每一个Partition其实都是一个文件</strong> ，收到消息后Kafka会把数据插入到文件末尾（虚框部分）。</p>
<p>这种方法有一个缺陷—— <strong>没有办法删除数据</strong> ，所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个Topic都有一个offset用来表示 <strong>读取到了第几条数据</strong> 。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g0hv3vgf8hj20h90633zc.jpg"></p>
<blockquote>
<p>上图中有两个消费者，Consumer1有两个offset分别对应Partition0、Partition1（假设每一个Topic一个Partition）；Consumer2有一个offset对应Partition2。这个offset是由客户端SDK负责保存的，Kafka的Broker完全无视这个东西的存在；一般情况下SDK会把它保存到zookeeper里面。(所以需要给Consumer提供zookeeper的地址)。</p>
<p>如果不删除硬盘肯定会被撑满，所以Kakfa提供了两种策略来删除数据。一是基于时间，二是基于partition文件大小。具体配置可以参看它的配置文档。</p>
</blockquote>
<p><strong>2、Memory Mapped Files（内存映射文件）</strong></p>
<p>即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以Kafka的数据并<strong>不是实时的写入硬盘</strong> ，它充分利用了现代操作系统<strong>分页存储</strong>来利用内存提高I&#x2F;O效率。</p>
<p>Memory Mapped Files(后面简称mmap)也被翻译成 <strong>内存映射文件</strong> ，在64位操作系统中一般可以表示20G的数据文件，它的工作原理是直接利用操作系统的Page来实现文件到物理内存的直接映射。完成映射之后你对物理内存的操作会被同步到硬盘上（操作系统在适当的时候）。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g0hv6pz4hij20j708iwf5.jpg"></p>
<p>使用这种方式可以获取很大的I&#x2F;O提升， <strong>省去了用户空间到内核空间</strong> 复制的开销（调用文件的read会把数据先放到内核空间的内存中，然后再复制到用户空间的内存中。）也有一个很明显的缺陷——不可靠， 写到mmap中的数据并没有被真正的写到硬盘，操作系统会在程序主动调用flush的时候才把数据真正的写到硬盘。 Kafka提供了一个参数——producer.type来控制是不是主动flush，如果Kafka写入到mmap之后就立即flush然后再返回Producer叫 <strong>同步</strong> (sync)；写入mmap之后立即返回Producer不调用flush叫 <strong>异步</strong> (async)。</p>
<p><strong>3、Kafka高效文件存储设计特点</strong></p>
<ul>
<li>Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易根据偏移量查找消息、定期清除和删除已经消费完成的数据文件，减少磁盘容量的占用；</li>
<li>采用稀疏索引存储的方式构建日志的偏移量索引文件，并将其映射至内存中，提高查找消息的效率，同时减少磁盘IO操作；并大幅降低index文件元数据占用空间大小。</li>
<li>Kafka将消息追加的操作逻辑变成为日志数据文件的顺序写入，极大的提高了磁盘IO的性能；</li>
</ul>
<h3 id="关于Kafka的经典面试知识点"><a href="#关于Kafka的经典面试知识点" class="headerlink" title="关于Kafka的经典面试知识点"></a>关于Kafka的经典面试知识点</h3><p>请参考：<a href="https://juejin.im/post/5c7bd09b6fb9a049ba424c15" rel="external nofollow noopener noreferrer" target="_blank">消息中间件如何实现每秒几十万的高并发写入？</a></p>
<p>1、页缓存技术 + 磁盘顺序写</p>
<p>2、零拷贝技术</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://www.seanxia.cn">SeanXia</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/efe707af.html">http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/efe707af.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.seanxia.cn" target="_blank">风雨欲来兮丶</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Kafka/">Kafka</a><a class="post-meta__tags" href="/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a></div><div class="post_share"><div class="social-share" data-image="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赏一个</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%BE%AE%E4%BF%A110%E5%85%83%E8%B5%9E%E8%B5%8F%E7%A0%81.jpg" target="_blank" rel="external nofollow noopener noreferrer"><img class="post-qr-code-img" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%BE%AE%E4%BF%A110%E5%85%83%E8%B5%9E%E8%B5%8F%E7%A0%81.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank" rel="external nofollow noopener noreferrer"><img class="post-qr-code-img" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/b6b65a25.html" title="流式处理Storm"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">流式处理Storm</div></div></a></div><div class="next-post pull-right"><a href="/Java/4ddce68d.html" title="spring框架总结"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">spring框架总结</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">SeanXia</div><div class="author-info__description">好奇搞怪,大数据分析,数据开发者一枚</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">72</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" href="https://github.com/Sdreamery" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Sdreamery" target="_blank" title="Github" rel="external nofollow noopener noreferrer"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sean.xs@foxmail.com" target="_blank" title="Email" rel="external nofollow noopener noreferrer"><i class="fas fa-envelope" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">Kafka 简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E6%9E%B6%E6%9E%84"><span class="toc-number">1.1.</span> <span class="toc-text">Kafka 架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E4%B8%AD%E5%85%B6%E4%BB%96%E5%85%B3%E9%94%AE%E8%AF%8D"><span class="toc-number">1.2.</span> <span class="toc-text">Kafka 中其他关键词</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka-%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.3.</span> <span class="toc-text">Kafka 的使用场景</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="toc-number">2.</span> <span class="toc-text">Kafka 集群部署</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Flume%E4%B8%8EKafka-%E6%95%B4%E5%90%88"><span class="toc-number">3.</span> <span class="toc-text">Flume与Kafka 整合</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E5%AF%B9%E6%AF%94-HDFS"><span class="toc-number">4.</span> <span class="toc-text">Kafka 对比 HDFS</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">5.</span> <span class="toc-text">Kafka的数据丢失和重复消费</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E7%94%9F%E5%8E%9F%E5%9B%A0"><span class="toc-number">5.1.</span> <span class="toc-text">发生原因</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E4%B8%A2%E5%A4%B1"><span class="toc-number">5.1.1.</span> <span class="toc-text">数据丢失</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">5.1.2.</span> <span class="toc-text">重复消费</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95"><span class="toc-number">5.2.</span> <span class="toc-text">解决办法</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="toc-number">6.</span> <span class="toc-text">Kafka 副本机制</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="toc-number">7.</span> <span class="toc-text">Kafka 数据存储</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E4%B8%AD%E5%88%86%E5%8C%BA-%E5%89%AF%E6%9C%AC%E7%9A%84%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E5%88%86%E6%9E%90"><span class="toc-number">7.1.</span> <span class="toc-text">Kafka中分区&#x2F;副本的日志文件存储分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E4%B8%AD%E6%97%A5%E5%BF%97%E7%B4%A2%E5%BC%95%E5%92%8C%E6%95%B0%E6%8D%AE%E6%96%87%E4%BB%B6%E7%9A%84%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84"><span class="toc-number">7.2.</span> <span class="toc-text">Kafka中日志索引和数据文件的存储结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E4%B8%ADMessage%E7%9A%84%E5%AD%98%E5%82%A8%E5%92%8C%E6%9F%A5%E6%89%BE%E8%BF%87%E7%A8%8B"><span class="toc-number">7.3.</span> <span class="toc-text">Kafka中Message的存储和查找过程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%9D%E8%80%83%E6%8B%93%E5%B1%95"><span class="toc-number">7.4.</span> <span class="toc-text">思考拓展</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Kafka%E4%BD%BF%E7%94%A8%E7%A3%81%E7%9B%98%E4%B9%9F%E5%8F%AF%E4%BB%A5%E9%AB%98%E6%95%88%E8%AF%BB%E5%86%99%E7%9A%84%E5%8E%9F%E5%9B%A0%EF%BC%9F"><span class="toc-number">7.4.1.</span> <span class="toc-text">Kafka使用磁盘也可以高效读写的原因？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8EKafka%E7%9A%84%E7%BB%8F%E5%85%B8%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-number">7.4.2.</span> <span class="toc-text">关于Kafka的经典面试知识点</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%85%B6%E4%BB%96/6d60df94.html" title="使用Git系统搭建GitLab">使用Git系统搭建GitLab</a><time datetime="2019-08-23T16:00:00.000Z" title="发表于 2019-08-24 00:00:00">2019-08-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%85%B6%E4%BB%96/7fb68dad.html" title="新浪微博图床迁移">新浪微博图床迁移</a><time datetime="2019-08-10T16:00:00.000Z" title="发表于 2019-08-11 00:00:00">2019-08-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/31afedf9.html" title="流式框架Flink（一）">流式框架Flink（一）</a><time datetime="2019-01-01T16:00:00.000Z" title="发表于 2019-01-02 00:00:00">2019-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1b90121.html" title="流式框架Flink（二）">流式框架Flink（二）</a><time datetime="2019-01-01T16:00:00.000Z" title="发表于 2019-01-02 00:00:00">2019-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1ca1f555.html" title="SparkMLlib 随机森林">SparkMLlib 随机森林</a><time datetime="2018-05-21T16:00:00.000Z" title="发表于 2018-05-22 00:00:00">2018-05-22</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By SeanXia</div><div class="framework-info"><span>框架 </span><a href="https://hexo.io" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" rel="external nofollow noopener noreferrer" target="_blank">Butterfly</a></div><div class="footer_custom_text"><a href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral" rel="external nofollow noopener noreferrer" target="_blank"><span>本网站由</span><img class="icp-icon" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/test/%E5%8F%88%E6%8B%8D%E4%BA%91_logo6.png"><span>提供 CSDN 加速/云存储服务</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>