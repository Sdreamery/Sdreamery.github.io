<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Spark计算框架（一） | 风雨欲来兮丶</title><meta name="author" content="SeanXia"><meta name="copyright" content="SeanXia"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。 Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark计算框架（一）">
<meta property="og:url" content="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/53ca074f.html">
<meta property="og:site_name" content="风雨欲来兮丶">
<meta property="og:description" content="Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。 Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg">
<meta property="article:published_time" content="2017-12-15T16:00:00.000Z">
<meta property="article:modified_time" content="2019-08-10T13:45:40.000Z">
<meta property="article:author" content="SeanXia">
<meta property="article:tag" content="spark">
<meta property="article:tag" content="分布式计算">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg"><link rel="shortcut icon" href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/LOGO%E9%80%8F%E6%98%8E.png"><link rel="canonical" href="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/53ca074f.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: SeanXia","link":"链接: ","source":"来源: 风雨欲来兮丶","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark计算框架（一）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2019-08-10 21:45:40'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="风雨欲来兮丶" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">72</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/butterfly.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="风雨欲来兮丶"><span class="site-name">风雨欲来兮丶</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark计算框架（一）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2017-12-15T16:00:00.000Z" title="发表于 2017-12-16 00:00:00">2017-12-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2019-08-10T13:45:40.000Z" title="更新于 2019-08-10 21:45:40">2019-08-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id data-flag-title="Spark计算框架（一）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/53ca074f.html#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Apache Spark 是专为大规模数据处理而设计的快速通用的计算引擎。</p>
<p>Spark是UC Berkeley AMP lab (加州大学伯克利分校的AMP实验室)所开源的类Hadoop MapReduce的通用并行框架。</p>
<span id="more"></span>
<h2 id="Spark介绍">Spark介绍</h2>
<p>Spark 基于 MapReduce 算法实现的分布式计算，拥有Hadoop MapReduce 所具有的优点；但不同于 MapReduce 的是 <code>Job中间输出和结果可以保存在内存中</code>，从而不再需要读写 HDFS，因此 Spark能更好地适用于数据挖掘与机器学习等需要迭代的 MapReduce 的算法。Spark 是 Scala 编写，方便快速编程。</p>
<p><strong>spark相比MapReduce重要的点：</strong></p>
<ul>
<li>spark的中间结果依然保存在内存中。</li>
<li>Apache Spark使用最先进的DAG（有向无环图）调度程序。</li>
</ul>
<p><code>这两个特点使得 spark 非常适用于迭代计算，在迭代计算中比MR快100倍。</code></p>
<h3 id="Spark的四大特性">Spark的四大特性</h3>
<p><strong>1、高效性</strong></p>
<p>在迭代计算中运行速度提高100倍。</p>
<p>Apache Spark使用最先进的DAG调度程序，查询优化程序和物理执行引擎，实现批量和流式数据的高性能。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g08jj9x7baj20ks05zq3a.jpg" alt></p>
<p><strong>2、易用性</strong></p>
<p>Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g08jki9tr8j20le05rt90.jpg" alt></p>
<p><strong>3、通用性</strong></p>
<p>Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g08jng52h5j20ld05mdgb.jpg" alt></p>
<p><strong>4、兼容性</strong></p>
<p>Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g08jp114raj20lg07xgmo.jpg" alt></p>
<h3 id="Spark运行模式">Spark运行模式</h3>
<p><strong>Local（本地）</strong></p>
<p>多用于本地测试，如在 eclipse，idea 中写程序测试等。</p>
<p><strong>standalone（spark自带）</strong></p>
<p>Spark自己可以给自己分配资源（master，worker）。</p>
<p><strong>Yarn</strong></p>
<p>Spark可以运行在yarn上面。</p>
<p><code>注意：要基于 Yarn 来进行资源调度，必须实现AppalicationMaster 接口，Spark 实现了这个接口，所以可以基于 Yarn。</code></p>
<p><strong>Mesos</strong></p>
<p>Spark可以运行在Mesos里面（Mesos 类似于yarn的一个资源调度框架）。</p>
<h3 id="Spark的组成">Spark的组成</h3>
<p>Spark组成(BDAS)：全称伯克利数据分析栈，通过大规模集成算法、机器、人之间展现大数据应用的一个平台。也是处理大数据、云计算、通信的技术解决方案。</p>
<p>它的主要组件有：</p>
<p><strong>SparkCore</strong>：将分布式数据抽象为弹性分布式数据集（RDD），实现了应用任务调度、RPC、序列化和压缩，并为运行在其上的上层组件提供API。</p>
<p><strong>SparkSQL</strong>：Spark Sql 是Spark来操作结构化数据的程序包，可以让我使用SQL语句的方式来查询数据，Spark支持 多种数据源，包含Hive表，parquest以及JSON等内容。</p>
<p><strong>SparkStreaming</strong>： 是Spark提供的实时数据进行流式计算的组件。</p>
<p><strong>MLlib</strong>：提供常用机器学习算法的实现库。</p>
<p><strong>GraphX</strong>：提供一个分布式图计算框架，能高效进行图计算。</p>
<p><strong>BlinkDB</strong>：用于在海量数据上进行交互式SQL的近似查询引擎。</p>
<p><strong>Tachyon</strong>：以内存为中心高容错的的分布式文件系统。</p>
<h3 id="应用场景">应用场景</h3>
<ul>
<li>
<p>Yahoo将Spark用在Audience Expansion中的应用，进行点击预测和即席查询等</p>
</li>
<li>
<p>淘宝技术团队使用了Spark来解决多次迭代的机器学习算法、高计算复杂度的算法等。应用于内容推荐、社区发现等</p>
</li>
<li>
<p>腾讯大数据精准推荐借助Spark快速迭代的优势，实现了在“数据实时采集、算法实时训练、系统实时预测”全流程实时并行高维算法，最终成功应用于广点通pCTR投放系统上。</p>
</li>
<li>
<p>优酷土豆将Spark应用于视频推荐(图计算)、广告业务，主要实现机器学习、图计算等迭代计算。</p>
</li>
</ul>
<h2 id="SparkCore">SparkCore</h2>
<h3 id="RDD">RDD</h3>
<p><strong>1、概念</strong></p>
<p>RDD(Resilient Distributed Dateset)，弹性分布式数据集，是Spark中最基本的数据抽象。</p>
<p><strong>2、RDD 的五大特性</strong></p>
<ol>
<li>RDD 是由一系列的 partition 组成的。</li>
<li>函数是作用在每一个 partition（split）上的。</li>
<li>RDD 之间有一系列的依赖关系。</li>
<li>分区器是作用在 K,V 格式的 RDD 上。</li>
<li>RDD 提供一系列最佳的计算位置。</li>
</ol>
<p><strong>3、RDD 理解图</strong></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g08jz1mcvzj20fe0cbaav.jpg" alt></p>
<p><strong>注意：</strong></p>
<p>1、textFile 方法底层封装的是读取 MR 读取文件的方式，读取文件之前先 split，默认 split 大小是一个 block 大小。</p>
<p>2、RDD  实际上不存储数据，这里方便理解，暂时理解为存储数据。</p>
<p>3、什么是 K,V 格式的 RDD?</p>
<ul>
<li>如果 RDD 里面存储的数据都是二元组对象，那么这个 RDD 我们<br>
就叫做 K,V 格式的 RDD。</li>
</ul>
<p>4、哪里体现 RDD 的弹性（容错）？</p>
<ul>
<li>partition 数量，大小没有限制，体现了 RDD 的弹性。</li>
<li>RDD 之间依赖关系，可以基于上一个 RDD 重新计算出 RDD，体现了容错。</li>
</ul>
<p>5、哪里体现 RDD 的分布式？</p>
<ul>
<li>RDD 是由 Partition 组成，partition 是分布在不同节点上的。</li>
</ul>
<p>6、RDD 提供计算最佳位置，体现了数据本地化。体现了大数据中 “计算移动数据不移动” 的理念。</p>
<h3 id="Spark任务执行原理">Spark任务执行原理</h3>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g08k2uotwkj20cv08kn02.jpg" alt></p>
<p>以上图中有四个机器节点，Driver 和 Worker 是启动在节点上的进程，运行在 JVM 中的进程。</p>
<ul>
<li>Driver 与集群节点之间有频繁的通信。</li>
<li>Driver 负责任务(tasks)的分发和结果的回收。任务的调度。如果 task 的计算结果非常大就不要回收了。会造成 oom。</li>
<li>Worker 是 Standalone 资源调度框架里面资源管理的从节点。也是 JVM 进程。</li>
<li>Master 是 Standalone 资源调度框架里面资源管理的主节点。也是 JVM 进程。</li>
</ul>
<h3 id="Spark代码流程">Spark代码流程</h3>
<p><strong>1、创建 SparkConf 对象</strong></p>
<ul>
<li>可以设置 Application name。</li>
<li>可以设置运行模式及资源需求。</li>
</ul>
<p><strong>2、创建 SparkContext 对象</strong></p>
<p><strong>3、基于 Spark 的上下文创建一个 RDD，对 RDD 进行处理。</strong></p>
<p><strong>4、应用程序中要有 Action 类算子来触发 Transformation 类算子执行。</strong></p>
<p><strong>5、关闭 Spark 上下文对象 SparkContext。</strong></p>
<h3 id="Transformations-转换算子">Transformations 转换算子</h3>
<p><strong>1、概念</strong></p>
<p>Transformations 类算子是一类算子（函数）叫做转换算子，如map，flatMap，reduceByKey 等。Transformations 算子是延迟执行，也叫懒加载执行。</p>
<p><strong>2、Transformation 类算子</strong></p>
<ul>
<li><strong>filter</strong><br>
过滤符合条件的记录数，true 保留，false 过滤掉。</li>
<li><strong>map</strong><br>
将一个 RDD 中的每个数据项，通过 map 中的函数映射变为一个新的元素。<br>
特点：输入一条，输出一条数据。</li>
<li><strong>flatMap</strong><br>
先 map 后 flat。与 map 类似，每个输入项可以映射为 0 到多个输出项。</li>
<li><strong>sample</strong><br>
随机抽样算子，根据传进去的小数按比例进行又放回或者无放回的抽样。</li>
<li><strong>reduceByKey</strong><br>
将相同的 Key 根据相应的逻辑进行处理。</li>
<li><strong>sortByKey/sortBy</strong><br>
作用在 K,V 格式的 RDD 上，对 key 进行升序或者降序排序。</li>
</ul>
<h3 id="Action-触发算子">Action 触发算子</h3>
<p><strong>1、概念</strong></p>
<p>Action 类算子也是一类算子（函数）叫做行动算子，如foreach,collect，count 等。Transformations 类算子是延迟执行，Action 类算子是触发执行。一个 application 应用程序中有几个 Action 类算子执行，就有几个 job 运行。</p>
<p><strong>2、Action 类算子</strong></p>
<ul>
<li><strong>count</strong><br>
返回数据集中的元素数。会在结果计算完成后回收到 Driver 端。</li>
<li><strong>take(n)</strong><br>
返回一个包含数据集前 n 个元素的集合。</li>
<li><strong>first</strong><br>
first=take(1)，返回数据集中的第一个元素。</li>
<li><strong>foreach</strong><br>
循环遍历数据集中的每个元素，运行相应的逻辑。</li>
<li><strong>collect</strong><br>
将计算结果转成集合回收到 Driver 端。</li>
</ul>
<h3 id="Demo01（WordCount）">Demo01（WordCount）</h3>
<p><strong>Scala实现</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">      * 几种运行方式：</span></span><br><span class="line"><span class="comment">      *   1.本地运行</span></span><br><span class="line"><span class="comment">      *   2.yarn</span></span><br><span class="line"><span class="comment">      *   3.standalone</span></span><br><span class="line"><span class="comment">      *   4.mesos</span></span><br><span class="line"><span class="comment">      */</span></span><br><span class="line">    conf.setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;wc&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> context = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lineRDD = context.textFile(<span class="string">&quot;./wc.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordRDD = lineRDD.flatMap(x =&gt; &#123;x.split(<span class="string">&quot; &quot;</span>)&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> <span class="type">KVRDD</span> = wordRDD.map(x =&gt; &#123;</span><br><span class="line">      println(<span class="string">&quot;=================&quot;</span>)</span><br><span class="line">      (x,<span class="number">1</span>)</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> resultRDD = <span class="type">KVRDD</span>.reduceByKey((x,y) =&gt; &#123;x+y&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sortRDD = resultRDD.sortBy(_._2,<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    sortRDD.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>Java实现</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">WordCount</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span>&#123;</span><br><span class="line">        <span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">        conf.setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;wc&quot;</span>);</span><br><span class="line">        <span class="type">JavaSparkContext</span> <span class="variable">context</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">        JavaRDD&lt;String&gt; javaRDD = context.textFile(<span class="string">&quot;./wc.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//        long count = javaRDD.count();</span></span><br><span class="line"><span class="comment">//        List&lt;String&gt; collect = javaRDD.collect();</span></span><br><span class="line"><span class="comment">//        List&lt;String&gt; take = javaRDD.take(5);</span></span><br><span class="line"><span class="comment">//        String first = javaRDD.first();</span></span><br><span class="line"></span><br><span class="line">        JavaRDD&lt;String&gt; wordRDD = javaRDD</span><br><span class="line">                .flatMap(<span class="keyword">new</span> <span class="title class_">FlatMapFunction</span>&lt;String, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Iterable&lt;String&gt; <span class="title function_">call</span><span class="params">(String line)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                String[] split = line.split(<span class="string">&quot; &quot;</span>);</span><br><span class="line">                List&lt;String&gt; list = Arrays.asList(split);</span><br><span class="line">                <span class="keyword">return</span> list;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; pairRDD = wordRDD</span><br><span class="line">                .mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;String, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title function_">call</span><span class="params">(String word)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>(word, <span class="number">1</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; resultRDD = pairRDD</span><br><span class="line">                .reduceByKey(<span class="keyword">new</span> <span class="title class_">Function2</span>&lt;Integer, Integer, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Integer <span class="title function_">call</span><span class="params">(Integer v1, Integer v2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> v1 + v2;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;Integer, String&gt; reverseRDD = resultRDD</span><br><span class="line">                .mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;String, Integer&gt;, Integer, String&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;Integer, String&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(tuple2._2, tuple2._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;Integer, String&gt; sortByKey = reverseRDD.sortByKey(<span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">        JavaPairRDD&lt;String, Integer&gt; result = sortByKey</span><br><span class="line">                .mapToPair(<span class="keyword">new</span> <span class="title class_">PairFunction</span>&lt;Tuple2&lt;Integer, String&gt;, String, Integer&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> Tuple2&lt;String, Integer&gt; <span class="title function_">call</span><span class="params">(Tuple2&lt;Integer, String&gt; tuple2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">Tuple2</span>&lt;&gt;(tuple2._2, tuple2._1);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        result.foreach(<span class="keyword">new</span> <span class="title class_">VoidFunction</span>&lt;Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">call</span><span class="params">(Tuple2&lt;String, Integer&gt; tuple2)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">                System.out.println(tuple2);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="控制算子">控制算子</h3>
<p>控制算子有三种，cache、persist、checkpoint，以上算子都可以将 RDD 持久化，持久化的单位是 partition。cache 和 persist 都是懒执行的，必须有一个 action 类算子触发执行。checkpoint 算子不仅能将 RDD 持久化到磁盘，还能切断 RDD 之间的依赖关系。</p>
<p><strong>1、cache</strong></p>
<p>默认将 RDD 的数据持久化到内存中。cache 是懒执行。</p>
<p><code>注意：cache() = persist()=persist(StorageLevel.Memory_Only)</code></p>
<ul>
<li>
<p>测试 cache 文件：</p>
<p>随机生成一个数据量比较大的测试文件：Test.txt</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">conf.setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;CacheTest&quot;</span>);</span><br><span class="line"><span class="type">JavaSparkContext</span> <span class="variable">jsc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">JavaRDD&lt;String&gt; lines = jsc.textFile(<span class="string">&quot;./Test.txt&quot;</span>);</span><br><span class="line"></span><br><span class="line">lines = lines.cache();</span><br><span class="line"><span class="type">long</span> <span class="variable">startTime</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line"><span class="type">long</span> <span class="variable">count</span> <span class="operator">=</span> lines.count();</span><br><span class="line"><span class="type">long</span> <span class="variable">endTime</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">System.out.println(<span class="string">&quot;共&quot;</span>+count+ <span class="string">&quot;条数据，&quot;</span>+<span class="string">&quot;初始化时间+cache时间+计算时间=&quot;</span>+(endTime-startTime));</span><br><span class="line"></span><br><span class="line"><span class="type">long</span> <span class="variable">countStartTime</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line"><span class="type">long</span> <span class="variable">countrResult</span> <span class="operator">=</span> lines.count();</span><br><span class="line"><span class="type">long</span> <span class="variable">countEndTime</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">System.out.println(<span class="string">&quot;共&quot;</span>+countrResult+ <span class="string">&quot;条数据，&quot;</span>+<span class="string">&quot;计算时间=&quot;</span>+ (countEndTime-countStartTime));</span><br><span class="line"></span><br><span class="line">jsc.stop();</span><br></pre></td></tr></table></figure>
<p><strong>2、persist</strong></p>
<p>可以指定持久化的级别。最常用的是 MEMORY_ONLY 和 MEMORY_AND_DISK。”_2”表示有副本数。</p>
<p>持久化级别如下：</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09qr5fatdj20ff0au77h.jpg" alt></p>
<blockquote>
<p><strong>cache 和 和 persist  的注意事项：</strong></p>
<ol>
<li>cache 和 persist 都是懒执行，必须有一个 action 类算子触发执行。</li>
<li>cache 和 persist 算子的返回值可以赋值给一个变量，在其他 job 中直接使用这个变量就是使用持久化的数据了。持久化的单位是 partition。</li>
<li>cache 和 persist 算子后不能立即紧跟 action 算子。</li>
</ol>
</blockquote>
<p><code>错误：rdd.cache().count() 返回的不是持久化的 RDD，而是一个数值了。</code></p>
<p><strong>3、checkpoint</strong></p>
<p>checkpoint 将 RDD 持久化到磁盘，还可以切断 RDD 之间的依赖关系。</p>
<ul>
<li>checkpoint 的执行原理：</li>
</ul>
<ol>
<li>
<p>当 RDD 的 job 执行完毕后，会从 finalRDD 从后往前回溯。</p>
</li>
<li>
<p>当回溯到某一个 RDD 调用了 checkpoint 方法，会对当前的 RDD 做一个标记。</p>
</li>
<li>
<p>Spark 框架会重新启动一个新的 job，从头开始计算到这个 RDD 的数据，将数据持久化到 HDFS 上。</p>
</li>
</ol>
<ul>
<li>
<p>优化：对 RDD 执行 checkpoint 之前，最好对这个 RDD 先执行 cache，这样新启动的 job 只需要将内存中的数据拷贝到 HDFS 上就可以，省去了重新计算这一步。</p>
</li>
<li>
<p>使用样例：</p>
</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="type">SparkConf</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>();</span><br><span class="line">conf.setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;checkpoint&quot;</span>);</span><br><span class="line"><span class="type">JavaSparkContext</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">JavaSparkContext</span>(conf);</span><br><span class="line">sc.setCheckpointDir(<span class="string">&quot;./checkpoint&quot;</span>);</span><br><span class="line">JavaRDD&lt;Integer&gt; parallelize = sc.parallelize(Arrays.asList(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>));</span><br><span class="line">parallelize.checkpoint();</span><br><span class="line">parallelize.count();</span><br><span class="line">sc.stop();</span><br></pre></td></tr></table></figure>
<h2 id="Spark集群搭建">Spark集群搭建</h2>
<h3 id="Standalone">Standalone</h3>
<p>1、下载安装包，解压</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09fp7f6lyj20ff078ab2.jpg" alt></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09fwqgx5sj20ey013a9u.jpg" alt></p>
<p>2、改名</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09fy5val2j20gb00zwea.jpg" alt></p>
<p>3、进入安装包的conf目录下，修改slaves.template文件，添加从节点。保存。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09fzkoucvj20cs01tt8i.jpg" alt></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09g0wb12fj20c802b0sj.jpg" alt></p>
<p>4、修改 <a href="http://spark-env.sh" rel="external nofollow noopener noreferrer" target="_blank">spark-env.sh</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">JAVA_HOME:配置 java_home 路径</span><br><span class="line">SPARK_MASTER_IP:master 的 ip</span><br><span class="line">SPARK_MASTER_PORT:提交任务的端口，默认是 7077</span><br><span class="line">SPARK_WORKER_CORES：每个 worker 从节点能够支配的 core 的个数</span><br><span class="line">SPARK_WORKER_MEMORY:每个 worker 从节点能够支配的内存数</span><br></pre></td></tr></table></figure>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09g4trqc5j20c90340sl.jpg" alt></p>
<p>5、同步到其他节点上</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09hj1zivuj20da0100si.jpg" alt></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09hjltg8dj20d80150si.jpg" alt></p>
<p>6、启动集群</p>
<p>进入 sbin 目录下，执行当前目录下的./start-all.sh</p>
<p>查看各节点进程：</p>
<p><code>主节点</code></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09migl9m8j208t02edfn.jpg" alt></p>
<p><code>从节点</code></p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09mj7ochvj209k028dfn.jpg" alt></p>
<p>7、启动Spark WEB 界面</p>
<p>使用提交任务的节点。</p>
<p><strong>注意：</strong></p>
<p>8080 是 Spark WEBUI 界面的端口，7077 是 Spark 任务提交的端口。</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09hsdnrk8j20z40a3gme.jpg" alt></p>
<h3 id="Yarn">Yarn</h3>
<p>1、1，2，3，4，5，7 步同 standalone。</p>
<p>2、在客户端中配置：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">HADOOP_CONF_DIR=/usr/soft/hadoop-2.6.5</span><br></pre></td></tr></table></figure>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09hzz1bi2j20ds0490sn.jpg" alt></p>
<h3 id="测试">测试</h3>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09m7fj9rwj209x0a8wes.jpg" alt></p>
<p>进到spark安装包的bin目录下，里面又一个脚本：spark-submit</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09mfgk55xj20cj02it8k.jpg" alt></p>
<p>然后执行以下命令提交测试任务。</p>
<p><strong>Standalone 提交命令：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./spark-submit --master spark://sean01:7077 --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 100</span><br></pre></td></tr></table></figure>
<p>从日志中可以查看到结果：</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09ml388fcj20bf02mdfq.jpg" alt></p>
<p>通过访问WEB端也能看到 &gt;&gt; sean01:8080</p>
<p><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g09mlt2b1ej20zu0a9dgq.jpg" alt></p>
<p><strong>YARN 提交命令：</strong></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./spark-submit --master yarn --class org.apache.spark.examples.SparkPi ../lib/spark-examples-1.6.0-hadoop2.6.0.jar 10</span><br></pre></td></tr></table></figure>
<p>提交之前要先关闭Standalone模式</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">进入 sbin 目录下，执行./stop-all.sh</span><br></pre></td></tr></table></figure>
<p>然后启动Hadoop集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">先启动Zookeeper：zkServer.sh start</span><br><span class="line">再启动Hadoop：start-all.sh</span><br></pre></td></tr></table></figure>
<p>最后提交spark任务：</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://www.seanxia.cn">SeanXia</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/53ca074f.html">http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/53ca074f.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.seanxia.cn" target="_blank">风雨欲来兮丶</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a><a class="post-meta__tags" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97/">分布式计算</a></div><div class="post_share"><div class="social-share" data-image="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赏一个</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%BE%AE%E4%BF%A110%E5%85%83%E8%B5%9E%E8%B5%8F%E7%A0%81.jpg" target="_blank" rel="external nofollow noopener noreferrer"><img class="post-qr-code-img" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%BE%AE%E4%BF%A110%E5%85%83%E8%B5%9E%E8%B5%8F%E7%A0%81.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank" rel="external nofollow noopener noreferrer"><img class="post-qr-code-img" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/de384fb5.html" title="Spark计算框架（二）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark计算框架（二）</div></div></a></div><div class="next-post pull-right"><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/b6b65a25.html" title="流式处理Storm"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">流式处理Storm</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/9326ece7.html" title="Spark计算框架（三）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-01-08</div><div class="title">Spark计算框架（三）</div></div></a></div><div><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/de384fb5.html" title="Spark计算框架（二）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2017-12-19</div><div class="title">Spark计算框架（二）</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">SeanXia</div><div class="author-info__description">好奇搞怪,大数据分析,数据开发者一枚</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">72</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" href="https://github.com/Sdreamery" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Sdreamery" target="_blank" title="Github" rel="external nofollow noopener noreferrer"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sean.xs@foxmail.com" target="_blank" title="Email" rel="external nofollow noopener noreferrer"><i class="fas fa-envelope" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">Spark介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E7%9A%84%E5%9B%9B%E5%A4%A7%E7%89%B9%E6%80%A7"><span class="toc-number">1.1.</span> <span class="toc-text">Spark的四大特性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.</span> <span class="toc-text">Spark运行模式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E7%9A%84%E7%BB%84%E6%88%90"><span class="toc-number">1.3.</span> <span class="toc-text">Spark的组成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.4.</span> <span class="toc-text">应用场景</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SparkCore"><span class="toc-number">2.</span> <span class="toc-text">SparkCore</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD"><span class="toc-number">2.1.</span> <span class="toc-text">RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E4%BB%BB%E5%8A%A1%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.</span> <span class="toc-text">Spark任务执行原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark%E4%BB%A3%E7%A0%81%E6%B5%81%E7%A8%8B"><span class="toc-number">2.3.</span> <span class="toc-text">Spark代码流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Transformations-%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="toc-number">2.4.</span> <span class="toc-text">Transformations 转换算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Action-%E8%A7%A6%E5%8F%91%E7%AE%97%E5%AD%90"><span class="toc-number">2.5.</span> <span class="toc-text">Action 触发算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Demo01%EF%BC%88WordCount%EF%BC%89"><span class="toc-number">2.6.</span> <span class="toc-text">Demo01（WordCount）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A7%E5%88%B6%E7%AE%97%E5%AD%90"><span class="toc-number">2.7.</span> <span class="toc-text">控制算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA"><span class="toc-number">3.</span> <span class="toc-text">Spark集群搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Standalone"><span class="toc-number">3.1.</span> <span class="toc-text">Standalone</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Yarn"><span class="toc-number">3.2.</span> <span class="toc-text">Yarn</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">3.3.</span> <span class="toc-text">测试</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%85%B6%E4%BB%96/6d60df94.html" title="使用Git系统搭建GitLab">使用Git系统搭建GitLab</a><time datetime="2019-08-23T16:00:00.000Z" title="发表于 2019-08-24 00:00:00">2019-08-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%85%B6%E4%BB%96/7fb68dad.html" title="新浪微博图床迁移">新浪微博图床迁移</a><time datetime="2019-08-10T16:00:00.000Z" title="发表于 2019-08-11 00:00:00">2019-08-11</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/31afedf9.html" title="流式框架Flink（一）">流式框架Flink（一）</a><time datetime="2019-01-01T16:00:00.000Z" title="发表于 2019-01-02 00:00:00">2019-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1b90121.html" title="流式框架Flink（二）">流式框架Flink（二）</a><time datetime="2019-01-01T16:00:00.000Z" title="发表于 2019-01-02 00:00:00">2019-01-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1ca1f555.html" title="SparkMLlib 随机森林">SparkMLlib 随机森林</a><time datetime="2018-05-21T16:00:00.000Z" title="发表于 2018-05-22 00:00:00">2018-05-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/butterfly.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By SeanXia</div><div class="framework-info"><span>框架 </span><a href="https://hexo.io" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" rel="external nofollow noopener noreferrer" target="_blank">Butterfly</a></div><div class="footer_custom_text"><a href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral" rel="external nofollow noopener noreferrer" target="_blank"><span>本网站由</span><img class="icp-icon" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/test/%E5%8F%88%E6%8B%8D%E4%BA%91_logo6.png"><span>提供 CSDN 加速/云存储服务</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: '6426e1d3285a84ce7134',
      clientSecret: '30e13e4623fd734097033c1edb8c8af6934f5c88',
      repo: 'Sdreamery.github.io',
      owner: 'Sdreamery',
      admin: ['Sdreamery'],
      id: 'f425f8deffe253c27062bf9e45362067',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>