<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SparkMLlib 逻辑回归 | 风雨欲来兮丶</title><meta name="author" content="SeanXia"><meta name="copyright" content="SeanXia"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="逻辑回归是预测分类响应的常用方法。这是广义线性模型的一个特例，可以预测结果的概率。 在spark.ml逻辑回归中，可以使用二项逻辑回归来预测二元结果，或者可以使用多项逻辑回归来预测多类结果。使用该family 参数在这两种算法之间进行选择，或者保持不设置，Spark将推断出正确的变量。">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkMLlib 逻辑回归">
<meta property="og:url" content="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/a4a0dc78.html">
<meta property="og:site_name" content="风雨欲来兮丶">
<meta property="og:description" content="逻辑回归是预测分类响应的常用方法。这是广义线性模型的一个特例，可以预测结果的概率。 在spark.ml逻辑回归中，可以使用二项逻辑回归来预测二元结果，或者可以使用多项逻辑回归来预测多类结果。使用该family 参数在这两种算法之间进行选择，或者保持不设置，Spark将推断出正确的变量。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2018-05-14T16:00:00.000Z">
<meta property="article:modified_time" content="2024-07-11T14:51:55.618Z">
<meta property="article:author" content="SeanXia">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="逻辑回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/LOGO%E9%80%8F%E6%98%8E.png"><link rel="canonical" href="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/a4a0dc78.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":300},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: SeanXia","link":"链接: ","source":"来源: 风雨欲来兮丶","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SparkMLlib 逻辑回归',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-11 22:51:55'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 7.3.0"><link rel="alternate" href="/atom.xml" title="风雨欲来兮丶" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">72</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="风雨欲来兮丶"><span class="site-name">风雨欲来兮丶</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">SparkMLlib 逻辑回归</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2018-05-14T16:00:00.000Z" title="发表于 2018-05-15 00:00:00">2018-05-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-07-11T14:51:55.618Z" title="更新于 2024-07-11 22:51:55">2024-07-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>25分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id data-flag-title="SparkMLlib 逻辑回归"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/a4a0dc78.html#post-comment"><span class="gitalk-comment-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>逻辑回归是预测分类响应的常用方法。这是<a href="https://en.wikipedia.org/wiki/Generalized_linear_model" rel="external nofollow noopener noreferrer" target="_blank">广义线性模型的</a>一个特例，可以预测结果的概率。</p>
<p>在<code>spark.ml</code>逻辑回归中，可以使用二项逻辑回归来预测二元结果，或者可以使用多项逻辑回归来预测多类结果。使用该<code>family</code> 参数在这两种算法之间进行选择，或者保持不设置，Spark将推断出正确的变量。</p>
<span id="more"></span>
<h2 id="逻辑回归的基本概念">逻辑回归的基本概念</h2>
<p>逻辑斯蒂回归（logistic regression）是统计学习中的经典分类方法，属于对数线性模型。logistic回归的因变量可以是二分类的，也可以是多分类的。</p>
<p><strong>逻辑回归与线性回归的区别：</strong></p>
<p>线性回归中 y 的值域在[-∞,+∞]，不能很好的表示；</p>
<p>逻辑回归通过 sigmod 函数将线性回归作为一个系数传进来，值域被映射为[0,1]，然后比如大于0.5为一类，小于0.5为一类</p>
<p><strong>补充：</strong></p>
<p>1、逻辑回归本质是求解二分类问题，一般所谓的预测就是分类；</p>
<p>2、所有的多分类的问题都可以转化为多个二分类的问题；</p>
<p>3、在Spark MLlib中二分类的话，1为正例，0为负例。</p>
<p>归结一句话就是：<strong>逻辑回归是一种线性有监督分类模型</strong>。</p>
<p><strong>逻辑回归的公式：</strong></p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(z)=\frac{1}{1+e^{-z}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.0908em;vertical-align:-0.7693em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6973em;"><span style="top:-2.989em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.7693em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>其中的 <strong>z=w_1x_1+w_2x_2+w_3x_3+…+w_nx_n+w_0</strong>，相当于多元线性回归。</p>
<p><strong>使用案例</strong></p>
<ul>
<li>
<p>在医学界，广泛应用于流行病学中，比如探索某个疾病的危险因素，根据危险因素预测疾病是否发生，与发生的概率。比如探讨胃癌，可以选择两组人群，一组是胃癌患者，一组是非胃癌患者。因变量是“是否胃癌”，这里“是”与“否”就是要研究的两个分类类别。自变量是两组人群的年龄，性别，饮食习惯，等等许多（可以根据经验假设），自变量可以是连续的，也可以是分类的。</p>
</li>
<li>
<p>在金融界，较为常见的是使用逻辑回归去预测贷款是否会违约，或放贷之前去估计贷款者未来是否会违约或违约的概率。</p>
</li>
<li>
<p>在消费行业中，也可以被用于预测某个消费者是否会购买某个商品，是否会购买会员卡，从而针对性得对购买概率大的用户发放广告，或代金券等等，进行精准营销。</p>
</li>
</ul>
<blockquote>
<p>前面我们说到过逻辑回归是一种用于分类的模型，就相当于y=f(x)，表明输入与输出（类别）的关系。最常见问题有如医生治病时的望、闻、问、切，之后判定病人是否生病或生了什么病，其中的望闻问切就是输入，即特征数据，判断是否生病就相当于获取因变量y，即分类结果。</p>
</blockquote>
<h2 id="案例：逻辑回归应用于二分类问题">案例：逻辑回归应用于二分类问题</h2>
<p><strong>个人信用预测</strong></p>
<p>保险公司在卖保险时，根据个人基本信息判断获赔概率</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gj7dhdcxj20gf04zjro.jpg">
<p><strong>二分类：</strong></p>
<p>要么A类，要么B类</p>
<p>比如人的身体状况为2中：1.健康；2生病</p>
<p>健康的可能性是1，生病的可能性就是0；<br>
健康的可能性是0.8，生病的可能性是0.2；<br>
健康的可能性是0.3，生病的可能性是0.7。</p>
<p>逻辑回归二分类：测试结果&gt;0.5为正例----获赔，测试结果y&lt;0.5为负例—不获赔</p>
<p><strong>训练集：健康状况训练集</strong></p>
<p><a href="https://github.com/Sdreamery/BigDataAPI/blob/master/SparkMLlib/%E5%81%A5%E5%BA%B7%E7%8A%B6%E5%86%B5%E8%AE%AD%E7%BB%83%E9%9B%86.txt" rel="external nofollow noopener noreferrer" target="_blank">健康状况训练集.txt</a></p>
<p>在案例问题中：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gjbit9g0j20cj01ka9z.jpg">
<p><strong>训练：</strong></p>
<p>确定w的过程，就是训练过程，spark的mllib已经做好了封装，只需调用即可</p>
<p>**singmod函数：**singmod函数为将线性变为非线性</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkabo4vhj205501qq2q.jpg">
<p><strong>函数图</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gke1zq9uj20br08pdg0.jpg">
<p><strong>如何判定结果：</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkekvlmhj20i705a3yq.jpg">
<p><strong>Spark MLlib中应用逻辑回归解决二分类问题的代码：</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;<span class="type">LogisticRegressionWithLBFGS</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogisticRegression1</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;spark&quot;</span>).setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">	<span class="comment">//加载基于SVM(支持向量机)算法的数据格式的文本文件：</span></span><br><span class="line">	<span class="comment">//底层将类似1 1:57 2:0 3:0 4:5 5:3 6:5转化为LabeledPoint </span></span><br><span class="line">    <span class="keyword">val</span> inputData:<span class="type">RDD</span>[<span class="type">LabeledPoint</span>] = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;健康状况训练集.txt&quot;</span>)</span><br><span class="line">	<span class="comment">//随机的将数据集切分为70%的训练集和30%的测试集，</span></span><br><span class="line">	<span class="comment">//训练集和测试集的选择：一般为训练集70%~80%，测试集为20%~30%</span></span><br><span class="line">	<span class="comment">//seed为随机种子，随机种子固定，则每次切分的训练集和测试集一样，</span></span><br><span class="line">	<span class="comment">//不信请使用下面的TestRandomSplit方法测试 </span></span><br><span class="line">	<span class="comment">//seed ：随机种子的作用：测试，固定每次切分相同的数据集和测试集，方便调试代码</span></span><br><span class="line">  <span class="keyword">val</span> splits:<span class="type">Array</span>[<span class="type">RDD</span>] = inputData.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>), seed = <span class="number">1</span>L)</span><br><span class="line">  <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">  <span class="comment">//使用LBFGS的优化方式创建一个逻辑回归的模型</span></span><br><span class="line">  <span class="comment">//也可以使用SGD的优化方式，一般来说LBFGS的优化方式更好</span></span><br><span class="line">  <span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">  <span class="comment">//训练模型：确定W的过程，即求方程组：</span></span><br><span class="line">  <span class="keyword">val</span> model = lr.run(trainingData)</span><br><span class="line">  <span class="comment">//testData为LabeledPoint：（标签，特征向量）</span></span><br><span class="line">  <span class="keyword">val</span> result = testData</span><br><span class="line">  <span class="comment">//把特征放入模型中测试：predict()方法底层封装了求解z=w(训练出来的模型)*x(特征)的过程</span></span><br><span class="line">  <span class="comment">//误差=真实y值-测试出的y值   取绝对值：测试正确为0，错误为1</span></span><br><span class="line">   .map&#123;point=&gt;<span class="type">Math</span>.abs(point.label-model.predict(point.features)) &#125;</span><br><span class="line">  <span class="comment">//错误率：result.mean()：将错误的数量求均值即错误率，正确率只是评价模型的一种指标</span></span><br><span class="line">    println(<span class="string">&quot;正确率=&quot;</span>+(<span class="number">1.0</span>-result.mean()))</span><br><span class="line">   <span class="comment">//将模型参数取出变为数组打印输出</span></span><br><span class="line">    println(model.weights.toArray.mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line">  <span class="comment">//打印输出截距w0，打印出来的结果全部会是0.0</span></span><br><span class="line">    println(model.intercept)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//测试方法</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestRandomSplit</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;TestRandomSplit&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(config = conf)</span><br><span class="line">    <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span> ,<span class="number">2</span> ,<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd = sc.parallelize(arr)</span><br><span class="line">    <span class="keyword">val</span> temp = rdd.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>,<span class="number">0.3</span>), seed = <span class="number">10</span>L)</span><br><span class="line">    <span class="keyword">val</span> (trainSet, testSet) = (temp(<span class="number">0</span>), temp(<span class="number">1</span>))</span><br><span class="line">    trainSet.foreach(println(_))</span><br><span class="line">    testSet.foreach(println(_))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>那么，上面的代码是中所谓的训练模型到底是怎么回事呢？</p>
<p><strong>请看图：</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkk907rwj20dj0963ys.jpg">
<p><strong>何为训练模型：</strong></p>
<p>从历史数据中，我们可以知道，个人的健康状况和用户的特征息息相关（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mn>6</mn></msub></mrow><annotation encoding="application/x-tex">x_0...x_6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">...</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>），因此通过 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 和初始 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>（初始值一般不全为0），通过逻辑回归将 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 带入 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>3</mn></msub><msub><mi>x</mi><mn>3</mn></msub><mo>+</mo><msub><mi>w</mi><mn>4</mn></msub><msub><mi>x</mi><mn>4</mn></msub><mo>+</mo><msub><mi>w</mi><mn>5</mn></msub><msub><mi>x</mi><mn>5</mn></msub><mo>+</mo><msub><mi>w</mi><mn>6</mn></msub><msub><mi>x</mi><mn>6</mn></msub></mrow><annotation encoding="application/x-tex">z=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5+w_6x_6</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">6</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 中，从而得到一个预测的健康状况。然后将真实的健康状况和预测的健康状况进行对比求错误，然后通过错误，不断的调整 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的值，反复迭代。直到错误接近于 0 的最优解，即错误最小的时刻。那么对应的模型 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 即为最后的模型。</p>
<p><strong>如何最快的求错误最小的时刻？-----调优，后面详细讲解</strong></p>
<p>有了训练出来的模型后就要测试模型的准确度，将测试数据带入模型即可求解：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkp0ndd8j20dc07baad.jpg">
<p><strong>换一种角度理解逻辑回归：逻辑回归分界线</strong></p>
<p>既然逻辑回归是线性有监督的<strong>分类模型</strong>，那么对于分类，分类的分界线在哪里呢？</p>
<p>对于二分类来说那肯定是0.5：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gks52yaaj20a002cq2v.jpg">
<p>很明显，求解分界线就是求解方程组：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gksqwor5j208l01u3yd.jpg">
<p>那么就有：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkt4hzvoj20cj034mx8.jpg">
<p>那么 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w_1x_1+w_2x_2+w_0=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">0</span></span></span></span>，对应于平面的一条直线，这条分解线就是分类线，将类别划分开来：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkuebeeyj20dv07xmxl.jpg">
<p>对应当其类比到高维空间，也是同样的原理。</p>
<p><strong>三维的需要一个平面作为分界线</strong>，四维的需要立体空间作为分界线 . . .</p>
<p><code>总结一句就是：</code></p>
<p>**求模型就是根据已知的数据集寻找分界线，平面，立体空间 . . . **</p>
<p><strong>对于二分类即找到一条直线将数据切分开，那么有无<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">W_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，对构成什么样的直线就很重要了</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gkx3n2y5j20bn07e74o.jpg">
<p>我们知道直线的公式是：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>k</mi><mi>x</mi><mo>+</mo><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">y=kx+w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，有无 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，即如图：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gky8o0sbj20dp06oaad.jpg">
<p><strong>那么总结一句：逻辑回归的本质:就是有w0 vs 无w0</strong></p>
<p><strong>现在我们用代码来测试一下：</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;LogisticRegressionWithLBFGS, LogisticRegressionWithSGD&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.MLUtils</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.RDD</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object LogisticRegression2 &#123;</span><br><span class="line">  def <span class="title function_">main</span><span class="params">(args: Array[String])</span> &#123;</span><br><span class="line">    <span class="type">val</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setAppName(<span class="string">&quot;spark&quot;</span>).setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkContext</span>(conf)</span><br><span class="line">    val inputData: RDD[LabeledPoint] = MLUtils.loadLibSVMFile(sc, <span class="string">&quot;w0测试数据.txt&quot;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">splits</span> <span class="operator">=</span> inputData.randomSplit(Array(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">    val (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    <span class="type">val</span> <span class="variable">lr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">    <span class="comment">// Spark Mllib封装的W0的设置，true为要有w0</span></span><br><span class="line">    lr.setIntercept(<span class="literal">true</span>)</span><br><span class="line">    val model=lr.run(trainingData)</span><br><span class="line">    val result=testData</span><br><span class="line">      .map&#123;point=&gt;Math.abs(point.label-model.predict(point.features)) &#125;</span><br><span class="line">    println(<span class="string">&quot;正确率=&quot;</span>+(<span class="number">1.0</span>-result.mean()))</span><br><span class="line">    println(model.weights.toArray.mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    println(model.intercept)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>是否应该有w0的测试数据：</p>
<p><a href="https://github.com/Sdreamery/BigDataAPI/blob/master/SparkMLlib/w0%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.txt" rel="external nofollow noopener noreferrer" target="_blank">w0测试数据.txt</a></p>
<p>测试过后你会明显的发现，有无正确率差异比较大。</p>
<h2 id="逻辑回归遇到线性不可分">逻辑回归遇到线性不可分</h2>
<p>刚刚还说逻辑回归是线性有监督的<strong>分类模型</strong>，既然是线性的，那么就会遇到线性不可分的问题：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gl1c9n2ej20ci08caaf.jpg">
<p>上面图中，你找不到一条线可以将其数据分开的，这个就是线性不可分问题，那么怎么解决呢？</p>
<p><strong>升维解决线性不可分的问题：</strong></p>
<p>一般是将已知的维度两两相乘，将其映射到高维的空间，那么就能找到一个平面将其分开。</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1gl2bqjo9j20g7061q35.jpg">
<p><strong>Spark Mllib中代码演示</strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;LogisticRegressionWithLBFGS, LogisticRegressionWithSGD&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.linalg.Vectors</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.LabeledPoint</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.MLUtils</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;SparkConf, SparkContext&#125;</span><br><span class="line"></span><br><span class="line">object LogisticRegression3 &#123;</span><br><span class="line">  def <span class="title function_">main</span><span class="params">(args: Array[String])</span> &#123;</span><br><span class="line">    <span class="type">val</span> <span class="variable">conf</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkConf</span>().setAppName(<span class="string">&quot;spark&quot;</span>).setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">sc</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">SparkContext</span>(conf)</span><br><span class="line">    <span class="comment">// 解决线性不可分我们来升维,升维有代价,计算复杂度变大了</span></span><br><span class="line">    <span class="type">val</span> <span class="variable">inputData</span> <span class="operator">=</span> MLUtils.loadLibSVMFile(sc, <span class="string">&quot;线性不可分数据集.txt&quot;</span>)</span><br><span class="line">      .map &#123; </span><br><span class="line">      <span class="comment">//将标注点取出，取出标签和特征</span></span><br><span class="line">    labelpoint =&gt;</span><br><span class="line">    <span class="comment">//标签</span></span><br><span class="line">        <span class="type">val</span> <span class="variable">label</span> <span class="operator">=</span> labelpoint.label</span><br><span class="line">      <span class="comment">//特征</span></span><br><span class="line">        <span class="type">val</span> <span class="variable">feature</span> <span class="operator">=</span> labelpoint.features</span><br><span class="line">      <span class="comment">//升维</span></span><br><span class="line">        <span class="type">val</span> <span class="variable">array</span> <span class="operator">=</span> Array(feature(<span class="number">0</span>), feature(<span class="number">1</span>), feature(<span class="number">0</span>) * feature(<span class="number">1</span>))</span><br><span class="line">     <span class="comment">//特征转为向量</span></span><br><span class="line">        <span class="type">val</span> <span class="variable">convertFeature</span> <span class="operator">=</span> Vectors.dense(array)</span><br><span class="line">     <span class="comment">//转为新的标注点</span></span><br><span class="line">        <span class="keyword">new</span> <span class="title class_">LabeledPoint</span>(label, convertFeature)</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="type">val</span> <span class="variable">splits</span> <span class="operator">=</span> inputData.randomSplit(Array(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">    val (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    <span class="type">val</span> <span class="variable">lr</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">    lr.setIntercept(<span class="literal">true</span>)</span><br><span class="line">    <span class="type">val</span> <span class="variable">model</span> <span class="operator">=</span> lr.run(trainingData)</span><br><span class="line">    <span class="type">val</span> <span class="variable">result</span> <span class="operator">=</span> testData</span><br><span class="line">      .map &#123; point =&gt; Math.abs(point.label - model.predict(point.features)) &#125;</span><br><span class="line">    println(<span class="string">&quot;正确率=&quot;</span> + (<span class="number">1.0</span> - result.mean()))</span><br><span class="line">    println(model.weights.toArray.mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    println(model.intercept)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>线性不可分数据集：</p>
<p><a href="https://github.com/Sdreamery/BigDataAPI/blob/master/SparkMLlib/%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86.txt" rel="external nofollow noopener noreferrer" target="_blank">线性不可分数据集.txt</a></p>
<p>测试过后你会明显的发现，升维过后正确率有明显的上升。</p>
<blockquote>
<p>在真实的生产中，轻易间一般不会出现线性不可分的升维问题，只有当出现准确率大幅度下降的时候猜测应该是线性不可分的问题时候，才特征两两组合升维，测试看效果。</p>
</blockquote>
<p><strong>总结：</strong></p>
<p>逻辑回归是一种线性 有监督分类模型，既然是有监督的，那么数据集中就应该有要预测的真实数据 y，既然是分类问题，那么有无 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">W_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 就很影响正确率，既然是线性模型，那么就会遇到线性不可分的问题，轻易间一般不会出现线性不可分的升维问题，当出现准确率大幅度下降的时候猜测应该是线性不可分的问题时候，才特征两两组合升维，测试看效果。</p>
<h2 id="关于阈值分析">关于阈值分析</h2>
<p>逻辑回归是求解二分类问题，那么分类的阈值是：结果&gt;0.5为正例，结果&lt;0.5为负例</p>
<blockquote>
<p>首先，先来思考一个问题？判断一个病人是否患癌症，判断情况（1）的风险大，还是判断情况（2）的风险大？</p>
<p>(1): 假如病人是癌症：</p>
<p>– 判断成不是癌症&lt;0.5</p>
<p>(2)假如病人是非癌症:</p>
<p>– 判断是癌症&gt;0.5</p>
<p>当然，很多人认为是第二种情况是糟糕的，但是第一种情况的后果更加的严重，病人承受的风险会加大，那么我们怎么通过调优模型来规避这样的风险呢？</p>
</blockquote>
<p>我们可以不使用逻辑回归的默认的分类阈值0.5，去除固定的阈值0.5，根据业务场景调整相应的情况调整阈值，比如说0.3， 虽然整体的错误率变大了，但是规避了一些不能接受的风险。</p>
<p>很明显，当你规避了一些不能接受的的风险以后，相应的模型的正确率下降了，错误率提高了。</p>
<p>好，现在使用之前的健康状况数据集，使用Spark MLlib的封装函数测试：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;<span class="type">LogisticRegressionWithLBFGS</span>, <span class="type">LogisticRegressionWithSGD</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogisticRegression4</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//去除阈值，规避风险</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;spark&quot;</span>).setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> inputData = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;健康状况训练集.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> splits = inputData.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">    <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">    lr.setIntercept(<span class="literal">true</span>)</span><br><span class="line"><span class="comment">//没去除固定的阈值的测试代码</span></span><br><span class="line"><span class="comment">//    val model = lr.run(trainingData)</span></span><br><span class="line"><span class="comment">//    val result = testData</span></span><br><span class="line"><span class="comment">//      .map&#123;point=&gt;Math.abs(point.label-model.predict(point.features)) &#125;</span></span><br><span class="line"><span class="comment">//    println(&quot;正确率=&quot;+(1.0-result.mean()))</span></span><br><span class="line"><span class="comment">//    println(model.weights.toArray.mkString(&quot; &quot;))</span></span><br><span class="line"><span class="comment">//    println(model.intercept)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//去除固定的阈值的代码</span></span><br><span class="line"><span class="comment">//Spark MLlib封装的去除固定的阈值的方法：.clearThreshold()</span></span><br><span class="line">    <span class="keyword">val</span> model = lr.run(trainingData).clearThreshold()</span><br><span class="line">    <span class="keyword">val</span> errorRate = testData.map&#123;p=&gt;</span><br><span class="line">      <span class="keyword">val</span> score = model.predict(p.features)</span><br><span class="line">      <span class="comment">// 去除了固定的阈值以后，需要设置阈值，然后进行比较</span></span><br><span class="line"><span class="comment">//癌症病人宁愿判断出得癌症也别错过一个得癌症的病人</span></span><br><span class="line">      <span class="keyword">val</span> result = score&gt;<span class="number">0.4</span> <span class="keyword">match</span> &#123;<span class="keyword">case</span> <span class="literal">true</span> =&gt; <span class="number">1</span> ; <span class="keyword">case</span> <span class="literal">false</span> =&gt; <span class="number">0</span>&#125;</span><br><span class="line">      <span class="type">Math</span>.abs(result-p.label)</span><br><span class="line">    &#125;.mean()</span><br><span class="line">    println(<span class="number">1</span>-errorRate)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="误差的优化：梯度下降分析">误差的优化：梯度下降分析</h2>
<p>前面提到了训练模型的过程，现在再来思考一下：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hd1hfy6rj20dx096t9e.jpg">
<p>图中的计算误差是一个反复迭代的过程，相应的调整模型W，那么怎么样才能求得最小的误差，获得最好的模型呢？其中我们可以使用一种叫梯度下降的方法（GD–<a href="http://spark.apache.org/docs/latest/mllib-optimization.html#stochastic-gradient-descent-sgd" rel="external nofollow noopener noreferrer" target="_blank">gradient descent</a>）</p>
<p>先看逻辑回归的误差函数：</p>
<p><strong>这个是其中的一个数据的误差函数：</strong><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdayttt0j20c203aglv.jpg"></p>
<p><strong>参数解释：</strong></p>
<p>T为线性代数中的矩阵转置</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hddh59qyj20cf03vdg3.jpg">
<p>那么，逻辑回归的 singmod 函数就变成了：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hde6ju3jj208602fmx1.jpg">
<p><strong>我们将类比下图看误差函数：</strong></p>
<p>对于一组参数：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdf0k3ybj208t01ja9x.jpg">
<p>对应误差：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdfnfg1oj20f1082dgw.jpg">
<p>那么对于所有的参数，总误差就是：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdgbisrrj20a107ggn1.jpg">
<p>很明显，找到参数对应的最矮的那堵墙的时候，就找到了最小的误差，上图最小的误差是参数4对应的误差，那么怎么才能以最快的速度求解最小的误差呢？在无数的参数对应的误差有点类似于一座山，现在考虑的就是怎么以最快的速度下山的问题？</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdhc0we5j20d808fn1y.jpg">
<p><strong>怎么以最快的速度下山呢？-----随机梯度下降法</strong></p>
<p>最快的下山的方式是每次都走 “最陡峭的道路”，最陡峭的道路很明显是最 “斜率” 的绝对值最大的，当走完上次最陡峭的路后，然后再找到最陡峭的路，直到走到谷底，这个过程就是随机梯度下降法。</p>
<p>SGD（随机梯度下降法）就是一种优化误差函数的方法：<code>以最快的速度求解最小误差</code>。</p>
<p><code>所谓的梯度就是斜率，每次都找到斜率的绝对值最大的，然后就能最快的求解最小的误差。</code></p>
<p>说白了就是求误差函数斜率为 0 的时刻！！！</p>
<p>那么训练的目的就是找到合适的 <strong>θ</strong> ，使得整体误差最小，即达到山底。<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdncjz7xj207201ht8k.jpg"></p>
<p>即最小化下式：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdpcvcrij20f402x74r.jpg">
<p>上面的误差函数被证明是凹函数（有极小值，无极大值），存在全局最优解，那么求解最小值的过程就是求解极值的过程。求极值就是让导数等于0，求解  <strong>θ</strong> ，也就是 <strong>w_0,w_1,w_2,…,w_n</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1he01tfinj20bz029jrh.jpg">
<p>但是从上函数可以看到几乎无法求解！</p>
<p>对于误差函数求导是比较困难，我们可以逆向思维，带入不同的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>w</mi><mn>2</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>w</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">w_0,w_1,w_2,...,w_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，反复的迭代直到找到斜率为0的时刻。</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hdzmnhr8j20bv02lt8v.jpg">
<p>那么就有：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1he0tesaej20ch02eq30.jpg">
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1he17vmiej20ch06774r.jpg">
<p>误差 <strong>J</strong> 随 <strong>θ</strong>变化图：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1he2m4ty7j20ab0670su.jpg">
<p>把误差函数当成一座山，梯度就是往前走时陡峭程度的数字化表现。</p>
<ol>
<li>绝对值越大，此时山越陡峭</li>
<li>梯度&gt;0，往前走是上山</li>
<li>梯度&lt;0，往前走是下山</li>
<li>梯度=0，山谷或者山峰</li>
</ol>
<p><strong>总结：</strong></p>
<p>梯度下降法就是一个找寻一座山最低谷的过程：</p>
<ol>
<li>如果当前往前走是上山，那么就后退；</li>
<li>如果当前往前走是下山，那么就前进；</li>
<li>不停的走，每走一步看下当前路况，决定下一步是前进还是后退，如此反复。</li>
</ol>
<p>公式表示为：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1he57ekslj208w03ct8s.jpg">
<p><strong>梯度下降示意图：</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1he7cuzdnj20b107274j.jpg">
<h2 id="鲁棒性调优">鲁棒性调优</h2>
<p><strong>鲁棒性出现的原因和目的：</strong></p>
<p>原始误差过于拟合，牺牲一些正确率来提高推广能力，防止过拟合！！！</p>
<p><strong>鲁棒性调优的作用：</strong></p>
<p>让你的模型更加的健壮，提高模型的通用性，推广能力、泛化能力。</p>
<p>对于表达式：<strong>w_1x_1+w_2x_2+w_0=0</strong>，对应于平面一条直线</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hehsy9x0j20a605qaa6.jpg">
<p>下述两个公式描述同一条直线，哪个好？</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1heigx4vyj208u03fjrf.jpg">
<p>在求 <strong>z</strong> 的过程中：<strong>z=w_1x_1+w_2x_2+w_3x_3+…+w_nx_n</strong>，如果 <strong>w</strong> 越大，求得的 <strong>z​</strong> 对逻辑回归的 sigmoid 函数影响就越大，抗干扰的能力就越小，小小的变化就会影响到最后的分类结果。</p>
<p><strong>总结：W 越小，模型的抗干扰能力越强。</strong></p>
<p>肯定，不是说 w 越小就越好，那怎么找到一组适合的最小W呢？</p>
<p><strong>加上正则提高你模型的推广能力-----设置lambda系数</strong></p>
<p>首先来看正则化公式：</p>
<p>L1正则&gt;0：<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1heoy9wgpj205l026web.jpg"></p>
<p>L2正则&gt;0：<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hepwl0nyj205i02dt8j.jpg"></p>
<p>为了提高模型的泛化能力（推广能力—不过拟合----适应更多的未来的新的数据，做到举一反三），需要重写误差函数，加入认为的惩罚系数 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">λ</span></span></span></span>：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hes7ozilj20c7043t8x.jpg">
<p>lambda( <strong>λ</strong> )是系数，区间在[0,1]：</p>
<p>lambda是 <strong>w</strong> 的权重，因此我们可以通过调整 lambda 的大小来决定是更加的看重模型的准确率还是更加的看重模型的推广能力，一般情况从经验来看，会把 lambda 设置为0.4</p>
<p>重写误差函数必定会牺牲一定的正确率，但是能提高模型的推广能力。<code>牺牲正确率来提高推广能力！！！</code></p>
<p><strong>正则 L1 和L2 怎么选择呢 – 区别？</strong></p>
<p>L1更加的倾向于使得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 要么取1，要么取0。也称为（lasso回归）稀疏编码，可用来降维。</p>
<p>L2更加的倾向于使得 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 整体偏小。也称为岭回归（rige回归）</p>
<p>一般L2更常用</p>
<p>关于回归技术，请参考：<a href="http://www.csdn.net/article/2015-08-19/2825492" rel="external nofollow noopener noreferrer" target="_blank">http://www.csdn.net/article/2015-08-19/2825492</a></p>
<p><strong>Spark MLlib中的实现L1或者L2代码测试：</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;<span class="type">LogisticRegressionWithLBFGS</span>, <span class="type">LogisticRegressionWithSGD</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.optimization.&#123;<span class="type">L1Updater</span>, <span class="type">SquaredL2Updater</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogisticRegression5</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//构建L1，L2</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;spark&quot;</span>).setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> inputData = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;健康状况训练集.txt&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> splits = inputData.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">    <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    <span class="comment">//只支持L2，LBFGS：每次调整考虑所有的数据</span></span><br><span class="line">   <span class="comment">//SGD：每次抽取一部分数据 -&gt; 随机梯度下降算法</span></span><br><span class="line">   <span class="comment">//LBFGS每次训练的时候取全数据进行 -&gt; 拟牛顿法</span></span><br><span class="line">    <span class="keyword">val</span> lr = <span class="keyword">new</span> <span class="type">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">  <span class="comment">//SGD---随机梯度下降法，随机的提取一部分数据，L1和L2都支持</span></span><br><span class="line">   <span class="comment">//val lr1 = new LogisticRegressionWithSGD()</span></span><br><span class="line">    lr.setIntercept(<span class="literal">true</span>)</span><br><span class="line">    <span class="comment">//设置L1或者L2，使用L1正则和L2正则的用处是便于提高模型的推广能力，相当于重写了误差函数</span></span><br><span class="line">   <span class="comment">// lr.optimizer.setUpdater(new L1Updater)</span></span><br><span class="line">   <span class="comment">//设置正则化L2，倾向于将W整体下降</span></span><br><span class="line">    lr.optimizer.setUpdater(<span class="keyword">new</span> <span class="type">SquaredL2Updater</span>)</span><br><span class="line">    <span class="comment">// 这块设置的是我们的lambda（惩罚系数）,越大越看重这个模型的推广能力,一般不会超过1,0.4是个比较好的值</span></span><br><span class="line">    lr.optimizer.setRegParam(<span class="number">0.4</span>)</span><br><span class="line">    <span class="keyword">val</span> model = lr.run(trainingData)</span><br><span class="line">    <span class="keyword">val</span> result=testData</span><br><span class="line">      .map&#123;point=&gt;<span class="type">Math</span>.abs(point.label-model.predict(point.features)) &#125;</span><br><span class="line">    println(<span class="string">&quot;正确率=&quot;</span>+(<span class="number">1.0</span>-result.mean()))</span><br><span class="line">    println(model.weights.toArray.mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    println(model.intercept)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="训练方法优化">训练方法优化</h2>
<p><strong>在 Spark MLlib 中 SGD 和 LBFGS 实现的区别：</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hg9v600rj20dq06kwfj.jpg">
<p>拟牛顿法(LBFGS)：<code>val lr = new LogisticRegressionWithLBFGS</code>，求二阶导数，相比SGD能更快更准确的收敛，每次迭代用到的训练集里面全部的数据，还可以做多分类。没有L1正则化，只有L2正则化（当不需要L1正则化时，强烈建议使用LBFGS）</p>
<p>随机梯度下降算法(SGD)：<code>val lr = new LogisticRegressWithSDG</code>，求一阶导数，相比基础梯度下降算法，它无需环顾四周360°，每次迭代随机抽取一部分训练集数据，求导，只能做二分类。L1和L2正则化都有。</p>
<p><strong>L- BFGS 为 SGD 的优化方法，它的训练速度比 SGD 快。</strong></p>
<h2 id="数值优化">数值优化</h2>
<p><strong>数值优化不会影响正确率，只会提升求解模型的速度！</strong></p>
<h3 id="数值优化一">数值优化一</h3>
<p>案例：某个地区的生态环境和动物数量的关系</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hfbv1ljkj20ff05u74g.jpg">
<p>观察上面的老虎的数量和麻雀的数量，可以知道老虎的数量和麻雀的数量差别很大，那么这样的输入数据会造成什么样的后果呢？</p>
<p>论点A：各个维度的输入如果在数值上差异很大，那么会引起正确的 <strong>w</strong> 在各个维度上数值差异很大。</p>
<p>论点B：找寻 <strong>w</strong> 的时候，对各个维度的调整基本上是按照同一个数量级来进行调整的（随机梯度下降的步长是一样的）。</p>
<p>发现 A 和 B 两条结论互相矛盾！！！</p>
<p>因为：<strong>0=w_1x_1+w_2x_2+w_3x_3+…+w_nx_n</strong>，当 <strong>x</strong> 的值差异很大的时候，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的值也差异很大。</p>
<p><strong>解决X的值差异很大的解决方案？------归一化</strong></p>
<p>归一化的方法：</p>
<p>（1）最大值最小值法 min-max标准化（Min-Max Normalization）</p>
<p>公式：<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hfj7djfjj206i029mx1.jpg"></p>
<p>**优点：**归一化的值一定在0~1之间</p>
<p>**缺点：**缺点是抗干扰能力弱，受离群值得影响比较大，导致0~1之间分布不均匀，中间容易					                  没有数据。</p>
<p>（2）方差归一化</p>
<p>**优点：**抗干扰能力强，和所有数据都有关，求标准差需要所有值的介入，重要有离群值的话，会被抑制下来。</p>
<p>**缺点：**是最终未必会落到0到1之间，牺牲归一化结果为代价提高稳定。</p>
<p>优化后：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hfpjhe6cj20f0045aae.jpg">
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hfp4h9uej20dl04aaa9.jpg">
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hfqgip1wj20hb05dt9g.jpg">
<p><code>总结：</code></p>
<blockquote>
<p>归一化的目的：归一化的目的是消除 <strong>x</strong> 之间的过大差异，从而消除 <strong>w</strong> 之间的过大差异，从而使得在训练（梯度下降法）的时候，使得 <strong>w​</strong> 之间的变化是同步、均匀的，从而使得你求解机器学习模型的速度加快！</p>
</blockquote>
<h3 id="数值优化二">数值优化二</h3>
<p>在训练模型，不断的调整 w 的时候：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hften3wxj2094048q31.jpg">
<p><strong>参数解释：</strong></p>
<p><strong>x_{i1}和 x_{i2}</strong> 是历史数据</p>
<p><strong>α</strong> 是调整的步长</p>
<p>A 是斜率</p>
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mi>t</mi></msup></mrow><annotation encoding="application/x-tex">w^t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7936em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7936em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span></span></span></span></span></span></span> 是 t 时刻的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">w^{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>是 t+1 时刻的 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span>，在不断的调整 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的时候，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">w^{t+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>是随着 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 同时变化的</p>
<p><strong>问题：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">w_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">w_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 只能朝一个方向变化。要么同时变大，要么同时变小</strong></p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hfzgcxo3j209p05wjri.jpg">
<p>如上图：以最快的速度从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">W_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mo>∗</mo></msub></mrow><annotation encoding="application/x-tex">W_*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1757em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>时刻，直线是最快的，调整 W 的时候，沿着蓝线的方向调整，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">W_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 减小，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mo>∗</mo></msub></mrow><annotation encoding="application/x-tex">W_*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1757em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 变大。</p>
<p>从上面的公式，我们知道数据 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 决定了 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的调整方向，让 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 有正有负就能调整 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的方向。</p>
<p>因此，解决方法：<strong>尽可能让 x 的各个维度取值上有正有负！</strong></p>
<p><strong>均值归一化—每个数量减去平均值</strong></p>
<p>方差归一化，均值归一化后：</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hg79k9pmj20fl04mglz.jpg">
<p>均值归一化后，沿着正确的方法调整 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的可能性更大，求解 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span></span></span></span> 的速度更快了</p>
<img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/7308598bgy1g1hg94ajcrj209i05mq2z.jpg">
<p>测试数据集：</p>
<p><a href="https://github.com/Sdreamery/BigDataAPI/blob/master/SparkMLlib/%E7%8E%AF%E5%A2%83%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE.txt" rel="external nofollow noopener noreferrer" target="_blank">环境分类数据.txt</a></p>
<p><strong>Spark MLlib实现数值优化代码：</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.classification.&#123;<span class="type">LogisticRegressionWithLBFGS</span>, <span class="type">LogisticRegressionWithSGD</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.feature.<span class="type">StandardScaler</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.regression.<span class="type">LabeledPoint</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.mllib.util.<span class="type">MLUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogisticRegression6</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">&quot;spark&quot;</span>).setMaster(<span class="string">&quot;local[3]&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> inputData = <span class="type">MLUtils</span>.loadLibSVMFile(sc, <span class="string">&quot;环境分类数据.txt&quot;</span>)</span><br><span class="line">   <span class="comment">//将特征值从源数据中抽取出来</span></span><br><span class="line">    <span class="keyword">val</span> vectors = inputData.map(_.features)</span><br><span class="line">    <span class="comment">//new一个标准归一化的实例，withMean=true(均值归一化), withStd=true(方差归一化)</span></span><br><span class="line">    <span class="keyword">val</span> scalerModel = <span class="keyword">new</span> <span class="type">StandardScaler</span>(withMean=<span class="literal">true</span>, withStd=<span class="literal">true</span>).fit(vectors)</span><br><span class="line">    <span class="keyword">val</span> normalizeInputData = inputData.map&#123;point =&gt;</span><br><span class="line">    <span class="comment">//将标签取出来</span></span><br><span class="line">     <span class="keyword">val</span> label = point.label</span><br><span class="line">      <span class="comment">//transform（稠密的向量），得到归一化的features </span></span><br><span class="line">      <span class="keyword">val</span> features = scalerModel.transform(point.features.toDense)</span><br><span class="line">     <span class="comment">//返回归一化的LabeledPoint</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">LabeledPoint</span>(label,features)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> splits = normalizeInputData.randomSplit(<span class="type">Array</span>(<span class="number">0.7</span>, <span class="number">0.3</span>))</span><br><span class="line">    <span class="keyword">val</span> (trainingData, testData) = (splits(<span class="number">0</span>), splits(<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> lr=<span class="keyword">new</span> <span class="type">LogisticRegressionWithLBFGS</span>()</span><br><span class="line">    lr.setIntercept(<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">val</span> model = lr.run(trainingData)</span><br><span class="line">    <span class="keyword">val</span> result=testData</span><br><span class="line">      .map&#123;point=&gt;<span class="type">Math</span>.abs(point.label-model.predict(point.features)) &#125;</span><br><span class="line">    println(<span class="string">&quot;正确率=&quot;</span>+(<span class="number">1.0</span>-result.mean()))</span><br><span class="line">    println(model.weights.toArray.mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    println(model.intercept)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="逻辑回归总结">逻辑回归总结</h2>
<p><strong>Logistic Regression：是一种线性有监督的分类模型。</strong></p>
<p><strong>1、设置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></strong></p>
<p>因为如果不设置 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">w_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 就会使得分界一定会穿过原点，这样就会使得计算的模型受到很大的局限性，设置方法：<code>lr.setIntercept(true)</code></p>
<p><strong>2、线性不可分</strong></p>
<p>遇到线性不可分的问题，需要升高维度，方法是让已有的维度俩俩相乘来构建更多的维度。</p>
   <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">.map &#123; labelpoint =&gt;</span><br><span class="line">	<span class="keyword">val</span> label = labelpoint.label</span><br><span class="line">	<span class="keyword">val</span> feature = labelpoint.features</span><br><span class="line">	<span class="keyword">val</span> array = <span class="type">Array</span>(feature(<span class="number">0</span>), feature(<span class="number">1</span>), feature(<span class="number">0</span>) * feature(<span class="number">1</span>))</span><br><span class="line">	<span class="keyword">val</span> convertFeature = <span class="type">Vectors</span>.dense(array)</span><br><span class="line">	<span class="keyword">new</span> <span class="type">LabeledPoint</span>(label, convertFeature)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>3、Threshold 阈值</strong></p>
<p>Threshold 阈值在 LR 里面默认是根据 0.5 来进行二分类的，为了去规避一些风险，我们可以去掉阈值，这样最后 LR 给的结果就是0到1之间的一个分值，我们可以根据分值，自己来定到底属于哪个类别</p>
<p>​    TradeOff：人为的去调整阈值，最后的正确率一定会下降</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> model = lr.run(trainingData).clearThreshold()</span><br></pre></td></tr></table></figure>
<p><strong>4、鲁棒性调优</strong></p>
<p>鲁棒性：模型的通用性，举一反三的能力，推广能力，泛化能力</p>
<p>尽可能在保证正确率的情况下，使得W越小越好！</p>
<p>好到什么程度？</p>
<p>定义 L1 正则化和 L2 正则化，然后重写误差函数让算法去减小误差的同时减小 L1 或 L2。</p>
<p>​    L1：有的趋近于1，有的趋近于0，稀疏编码</p>
<p>​    L2：整体的W同时变小，岭回归</p>
<p>​    TradeOff: 人为的去重写了误差函数，会导致最后的正确率一定会下降</p>
  <figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">lr.optimizer.setUpdater(<span class="keyword">new</span> <span class="type">SquaredL2Updater</span>)</span><br><span class="line">lr.optimizer.setRegParam(<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<p><strong>5、数值优化</strong></p>
<p>（1）方差归一化：会考虑到一组数里面的所有数据，就是每个数去除以方差</p>
<p>​	优点：就是会使得各个W基本数量级一致</p>
<p>​	缺点：所有的值未必都会落到0到1之间</p>
<p>（2）均值归一化：让找到最优的速度变快，让各个维度的数据有正有负，就会使得各个W在调整的时候有的变大有的变小</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> scalerModel = <span class="keyword">new</span> <span class="type">StandardScaler</span>(withMean=<span class="literal">true</span>, withStd=<span class="literal">true</span>).fit(vectors)</span><br></pre></td></tr></table></figure>
<p><strong>6、SGD 和 LBFGS的区别</strong></p>
<p>SGD：随机梯度下降法，每次迭代随机抽取一部分训练集数据，求导，只能做二分类。</p>
<p>LBFGS：拟牛顿法，速度比较快求二阶导数，每次迭代用到训练集里面所有的数据，还可能做多分类。</p>
<p>公司使用：直接使用 LBFGS</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> lr=<span class="keyword">new</span> <span class="type">LogisticRegressionWithSGD</span>()</span><br><span class="line"><span class="keyword">val</span> lr=<span class="keyword">new</span> <span class="type">LogisticRegressionWithLBFGS</span>()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://www.seanxia.cn">SeanXia</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/a4a0dc78.html">http://www.seanxia.cn/%E5%A4%A7%E6%95%B0%E6%8D%AE/a4a0dc78.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="external nofollow noopener noreferrer">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://www.seanxia.cn" target="_blank">风雨欲来兮丶</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><a class="post-meta__tags" href="/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">逻辑回归</a></div><div class="post_share"><div class="addtoany"><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_button_wechat"></a><a class="a2a_button_sina_weibo"></a><a class="a2a_button_email"></a><a class="a2a_button_copy_link"></a><a class="a2a_dd" href="https://www.addtoany.com/share" rel="external nofollow noopener noreferrer" target="_blank"></a></div></div><script async="async" src="https://static.addtoany.com/menu/page.js"></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>赏一个</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%BE%AE%E4%BF%A110%E5%85%83%E8%B5%9E%E8%B5%8F%E7%A0%81.jpg" target="_blank" rel="external nofollow noopener noreferrer"><img class="post-qr-code-img" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%BE%AE%E4%BF%A110%E5%85%83%E8%B5%9E%E8%B5%8F%E7%A0%81.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" target="_blank" rel="external nofollow noopener noreferrer"><img class="post-qr-code-img" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E6%94%AF%E4%BB%98%E5%AE%9D%E6%94%B6%E6%AC%BE%E7%A0%81.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1ca1f555.html" title="SparkMLlib 随机森林"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SparkMLlib 随机森林</div></div></a></div><div class="next-post pull-right"><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/30277e10.html" title="SparkMLlib线性回归"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">SparkMLlib线性回归</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1ca1f555.html" title="SparkMLlib 随机森林"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-05-22</div><div class="title">SparkMLlib 随机森林</div></div></a></div><div><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/30277e10.html" title="SparkMLlib线性回归"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-05-12</div><div class="title">SparkMLlib线性回归</div></div></a></div><div><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/31efd837.html" title="SparkMLlib Kmeans聚类"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-05-10</div><div class="title">SparkMLlib Kmeans聚类</div></div></a></div><div><a href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/800c7c16.html" title="SparkMLlib贝叶斯分类"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2018-05-06</div><div class="title">SparkMLlib贝叶斯分类</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/headpic.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">SeanXia</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">43</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">72</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" href="https://github.com/Sdreamery" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Sdreamery" target="_blank" title="Github" rel="external nofollow noopener noreferrer"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sean.xs@foxmail.com" target="_blank" title="Email" rel="external nofollow noopener noreferrer"><i class="fas fa-envelope" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">逻辑回归的基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%EF%BC%9A%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%BA%94%E7%94%A8%E4%BA%8E%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">2.</span> <span class="toc-text">案例：逻辑回归应用于二分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E9%81%87%E5%88%B0%E7%BA%BF%E6%80%A7%E4%B8%8D%E5%8F%AF%E5%88%86"><span class="toc-number">3.</span> <span class="toc-text">逻辑回归遇到线性不可分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E9%98%88%E5%80%BC%E5%88%86%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">关于阈值分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%AF%E5%B7%AE%E7%9A%84%E4%BC%98%E5%8C%96%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%88%86%E6%9E%90"><span class="toc-number">5.</span> <span class="toc-text">误差的优化：梯度下降分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%B2%81%E6%A3%92%E6%80%A7%E8%B0%83%E4%BC%98"><span class="toc-number">6.</span> <span class="toc-text">鲁棒性调优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E4%BC%98%E5%8C%96"><span class="toc-number">7.</span> <span class="toc-text">训练方法优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96"><span class="toc-number">8.</span> <span class="toc-text">数值优化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E4%B8%80"><span class="toc-number">8.1.</span> <span class="toc-text">数值优化一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E4%BC%98%E5%8C%96%E4%BA%8C"><span class="toc-number">8.2.</span> <span class="toc-text">数值优化二</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%80%BB%E7%BB%93"><span class="toc-number">9.</span> <span class="toc-text">逻辑回归总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/%E5%85%B6%E4%BB%96/6d60df94.html" title="使用Git系统搭建GitLab"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="使用Git系统搭建GitLab"></a><div class="content"><a class="title" href="/%E5%85%B6%E4%BB%96/6d60df94.html" title="使用Git系统搭建GitLab">使用Git系统搭建GitLab</a><time datetime="2019-08-23T16:00:00.000Z" title="发表于 2019-08-24 00:00:00">2019-08-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%85%B6%E4%BB%96/7fb68dad.html" title="新浪微博图床迁移"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="新浪微博图床迁移"></a><div class="content"><a class="title" href="/%E5%85%B6%E4%BB%96/7fb68dad.html" title="新浪微博图床迁移">新浪微博图床迁移</a><time datetime="2019-08-10T16:00:00.000Z" title="发表于 2019-08-11 00:00:00">2019-08-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/31afedf9.html" title="流式框架Flink（一）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="流式框架Flink（一）"></a><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/31afedf9.html" title="流式框架Flink（一）">流式框架Flink（一）</a><time datetime="2019-01-01T16:00:00.000Z" title="发表于 2019-01-02 00:00:00">2019-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1b90121.html" title="流式框架Flink（二）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="流式框架Flink（二）"></a><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1b90121.html" title="流式框架Flink（二）">流式框架Flink（二）</a><time datetime="2019-01-01T16:00:00.000Z" title="发表于 2019-01-02 00:00:00">2019-01-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1ca1f555.html" title="SparkMLlib 随机森林"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SparkMLlib 随机森林"></a><div class="content"><a class="title" href="/%E5%A4%A7%E6%95%B0%E6%8D%AE/1ca1f555.html" title="SparkMLlib 随机森林">SparkMLlib 随机森林</a><time datetime="2018-05-21T16:00:00.000Z" title="发表于 2018-05-22 00:00:00">2018-05-22</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By SeanXia</div><div class="framework-info"><span>框架 </span><a href="https://hexo.io" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" rel="external nofollow noopener noreferrer" target="_blank">Butterfly</a></div><div class="footer_custom_text"><a href="https://www.upyun.com/?utm_source=lianmeng&utm_medium=referral" rel="external nofollow noopener noreferrer" target="_blank"><span>本网站由</span><img class="icp-icon" src="https://seanxia.oss-cn-shanghai.aliyuncs.com/img/hexo/%E5%8F%88%E6%8B%8D%E4%BA%91_logo5.png"><span>提供 CSDN 加速/云存储服务</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: '6426e1d3285a84ce7134',
      clientSecret: '30e13e4623fd734097033c1edb8c8af6934f5c88',
      repo: 'Sdreamery.github.io',
      owner: 'Sdreamery',
      admin: ['Sdreamery'],
      id: '9c7afca493223518d226f079995add99',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Gitalk' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zindex="-1" mobile="true" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>